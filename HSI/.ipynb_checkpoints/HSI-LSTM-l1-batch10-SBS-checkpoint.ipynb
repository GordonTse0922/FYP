{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9ae2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score, mean_squared_error,mean_absolute_percentage_error,r2_score\n",
    "import tensorflow as tf\n",
    "import talib\n",
    "from tensorflow import keras\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad5f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_27789/3609140658.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data.drop('Volume',1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "Date                                                                 \n",
       "2018-01-02  30515.300781  30515.300781  30515.300781  30515.300781   \n",
       "2018-01-03  30560.900391  30560.900391  30560.900391  30560.900391   \n",
       "2018-01-04  30736.500000  30736.500000  30736.500000  30736.500000   \n",
       "2018-01-05  30814.599609  30814.599609  30814.599609  30814.599609   \n",
       "2018-01-08  30899.500000  30899.500000  30899.500000  30899.500000   \n",
       "...                  ...           ...           ...           ...   \n",
       "2021-12-27  23223.800781  23223.800781  23223.800781  23223.800781   \n",
       "2021-12-28  23280.599609  23280.599609  23280.599609  23280.599609   \n",
       "2021-12-29  23086.500000  23086.500000  23086.500000  23086.500000   \n",
       "2021-12-30  23112.000000  23112.000000  23112.000000  23112.000000   \n",
       "2021-12-31  23397.699219  23397.699219  23397.699219  23397.699219   \n",
       "\n",
       "               Adj Close  \n",
       "Date                      \n",
       "2018-01-02  30515.300781  \n",
       "2018-01-03  30560.900391  \n",
       "2018-01-04  30736.500000  \n",
       "2018-01-05  30814.599609  \n",
       "2018-01-08  30899.500000  \n",
       "...                  ...  \n",
       "2021-12-27  23223.800781  \n",
       "2021-12-28  23280.599609  \n",
       "2021-12-29  23086.500000  \n",
       "2021-12-30  23112.000000  \n",
       "2021-12-31  23397.699219  \n",
       "\n",
       "[822 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=yf.download('HSI','2018-01-01','2022-01-01')\n",
    "data.drop('Volume',1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781ab9b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['6day MA'] = data['Close'].rolling(window = 6).mean()\n",
    "data['12day MA'] = data['Close'].rolling(window = 12).mean()\n",
    "data['RSI'] = talib.RSI(data['Close'].values, timeperiod = 7)\n",
    "data['%R5'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 5)\n",
    "data['%R10'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 10)\n",
    "data['MI6']=talib.MOM(data['Close'],timeperiod=6)\n",
    "data['MI12']=talib.MOM(data['Close'],timeperiod=12)\n",
    "macd, macdsignal, macdhist = talib.MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "data['EMA12']=talib.EMA(data['Close'], timeperiod=12)\n",
    "data['EMA26']=talib.EMA(data['Close'],timeperiod=26)\n",
    "data['MACD']=macd\n",
    "data['TR']=talib.TRANGE(data['High'],data['Low'],data['Close'])\n",
    "data['OSC6']=talib.CMO(data['Close'], timeperiod=6)\n",
    "data['OSC12']=talib.CMO(data['Close'], timeperiod=12)\n",
    "data['Prediction']=data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68726853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_27789/1968958741.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  importance=mutual_info_regression(data.drop('Prediction',1),data['Prediction'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "importance=mutual_info_regression(data.drop('Prediction',1),data['Prediction'])\n",
    "feature_importances=pd.Series(importance,data.columns[0:len(data.columns)-1])\n",
    "feature_importances.plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1036fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_27789/244261082.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data.drop(['OSC12','OSC6','MI12','MI6','%R10','%R5','RSI','TR'],1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>6day MA</th>\n",
       "      <th>12day MA</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23113.366862</td>\n",
       "      <td>23494.916829</td>\n",
       "      <td>23397.249467</td>\n",
       "      <td>23841.259170</td>\n",
       "      <td>-444.009703</td>\n",
       "      <td>23223.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23071.416992</td>\n",
       "      <td>23430.491862</td>\n",
       "      <td>23370.565054</td>\n",
       "      <td>23795.521511</td>\n",
       "      <td>-424.956457</td>\n",
       "      <td>23280.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23086.083659</td>\n",
       "      <td>23349.300130</td>\n",
       "      <td>23356.724216</td>\n",
       "      <td>23757.379148</td>\n",
       "      <td>-400.654932</td>\n",
       "      <td>23086.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23143.016927</td>\n",
       "      <td>23273.533529</td>\n",
       "      <td>23315.151260</td>\n",
       "      <td>23707.684396</td>\n",
       "      <td>-392.533137</td>\n",
       "      <td>23112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23166.466797</td>\n",
       "      <td>23203.316895</td>\n",
       "      <td>23283.897220</td>\n",
       "      <td>23663.559626</td>\n",
       "      <td>-379.662406</td>\n",
       "      <td>23397.699219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "Date                                                                 \n",
       "2021-12-23  23193.599609  23193.599609  23193.599609  23193.599609   \n",
       "2021-12-27  23223.800781  23223.800781  23223.800781  23223.800781   \n",
       "2021-12-28  23280.599609  23280.599609  23280.599609  23280.599609   \n",
       "2021-12-29  23086.500000  23086.500000  23086.500000  23086.500000   \n",
       "2021-12-30  23112.000000  23112.000000  23112.000000  23112.000000   \n",
       "\n",
       "               Adj Close       6day MA      12day MA         EMA12  \\\n",
       "Date                                                                 \n",
       "2021-12-23  23193.599609  23113.366862  23494.916829  23397.249467   \n",
       "2021-12-27  23223.800781  23071.416992  23430.491862  23370.565054   \n",
       "2021-12-28  23280.599609  23086.083659  23349.300130  23356.724216   \n",
       "2021-12-29  23086.500000  23143.016927  23273.533529  23315.151260   \n",
       "2021-12-30  23112.000000  23166.466797  23203.316895  23283.897220   \n",
       "\n",
       "                   EMA26        MACD    Prediction  \n",
       "Date                                                \n",
       "2021-12-23  23841.259170 -444.009703  23223.800781  \n",
       "2021-12-27  23795.521511 -424.956457  23280.599609  \n",
       "2021-12-28  23757.379148 -400.654932  23086.500000  \n",
       "2021-12-29  23707.684396 -392.533137  23112.000000  \n",
       "2021-12-30  23663.559626 -379.662406  23397.699219  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['OSC12','OSC6','MI12','MI6','%R10','%R5','RSI','TR'],1,inplace=True)\n",
    "data_val=data[-100:]\n",
    "data=data[:-100]\n",
    "data_val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b49c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "sc_y= MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64aa428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Prediction']=data['Close'].shift(-1)\n",
    "# data.dropna(inplace=True)\n",
    "# data_val=data[-100:]\n",
    "# data=data[:-100]\n",
    "# data_val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb6680b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619, 1, 10) (619,) (69, 1, 10) (69,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_27789/2884921294.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X=sc.fit_transform(data.drop('Prediction',1))\n"
     ]
    }
   ],
   "source": [
    "X=sc.fit_transform(data.drop('Prediction',1))\n",
    "#X=data.drop('Prediction',1).values\n",
    "# Y = data[\"Prediction\"].values.reshape(-1,1)\n",
    "# Y = sc_y.fit_transform(data[\"Prediction\"].values.reshape(-1,1))\n",
    "Y=np.array(data[\"Close\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8c5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:38:15.537187: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-10 00:38:15.537372: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model_tech = keras.Sequential()\n",
    "model_tech.add(keras.layers.LSTM(\n",
    "  units=128,\n",
    "  input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    return_sequences=True,\n",
    "    kernel_regularizer=regularizers.l1(0.01)\n",
    "   \n",
    "))\n",
    "model_tech.add(keras.layers.LSTM(units=64))\n",
    "model_tech.add(keras.layers.Dense(units=64))\n",
    "model_tech.add(keras.layers.Dense(units=1))\n",
    "model_tech.compile(\n",
    "  loss='mean_squared_error',\n",
    "  optimizer=keras.optimizers.Adam(0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d66a4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:38:16.543086: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-10 00:38:17.858282: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:38:18.261200: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:38:18.771305: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:38:20.066844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:38:20.572083: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 0s - loss: 544897024.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:38:22.559389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:38:22.637167: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:38:22.659137: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 7s 66ms/step - loss: 544897024.0000 - val_loss: 522125920.0000\n",
      "Epoch 2/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 543564160.0000 - val_loss: 519745920.0000\n",
      "Epoch 3/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 540686400.0000 - val_loss: 516863456.0000\n",
      "Epoch 4/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 537440192.0000 - val_loss: 513497664.0000\n",
      "Epoch 5/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 533595488.0000 - val_loss: 509514240.0000\n",
      "Epoch 6/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 529087264.0000 - val_loss: 504890016.0000\n",
      "Epoch 7/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 523903904.0000 - val_loss: 499622176.0000\n",
      "Epoch 8/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 518046688.0000 - val_loss: 493715200.0000\n",
      "Epoch 9/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 511523680.0000 - val_loss: 487179808.0000\n",
      "Epoch 10/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 504350304.0000 - val_loss: 480035680.0000\n",
      "Epoch 11/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 496550400.0000 - val_loss: 472307936.0000\n",
      "Epoch 12/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 488152096.0000 - val_loss: 464021248.0000\n",
      "Epoch 13/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 479185504.0000 - val_loss: 455202400.0000\n",
      "Epoch 14/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 469684576.0000 - val_loss: 445889088.0000\n",
      "Epoch 15/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 459686784.0000 - val_loss: 436123776.0000\n",
      "Epoch 16/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 449229536.0000 - val_loss: 425939968.0000\n",
      "Epoch 17/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 438349408.0000 - val_loss: 415370848.0000\n",
      "Epoch 18/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 427083776.0000 - val_loss: 404452032.0000\n",
      "Epoch 19/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 415471232.0000 - val_loss: 393220384.0000\n",
      "Epoch 20/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 403550976.0000 - val_loss: 381713312.0000\n",
      "Epoch 21/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 391362656.0000 - val_loss: 369968352.0000\n",
      "Epoch 22/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 378945792.0000 - val_loss: 358023232.0000\n",
      "Epoch 23/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 366340064.0000 - val_loss: 345915328.0000\n",
      "Epoch 24/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 353584608.0000 - val_loss: 333681728.0000\n",
      "Epoch 25/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 340718208.0000 - val_loss: 321359072.0000\n",
      "Epoch 26/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 327779232.0000 - val_loss: 308983264.0000\n",
      "Epoch 27/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 314804768.0000 - val_loss: 296589440.0000\n",
      "Epoch 28/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 301831680.0000 - val_loss: 284211840.0000\n",
      "Epoch 29/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 288895200.0000 - val_loss: 271883744.0000\n",
      "Epoch 30/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 276029888.0000 - val_loss: 259637200.0000\n",
      "Epoch 31/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 263268880.0000 - val_loss: 247503168.0000\n",
      "Epoch 32/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 250643936.0000 - val_loss: 235511232.0000\n",
      "Epoch 33/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 238185472.0000 - val_loss: 223689600.0000\n",
      "Epoch 34/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 225922432.0000 - val_loss: 212064896.0000\n",
      "Epoch 35/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 213881856.0000 - val_loss: 200662352.0000\n",
      "Epoch 36/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 202089488.0000 - val_loss: 189505424.0000\n",
      "Epoch 37/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 190569008.0000 - val_loss: 178616032.0000\n",
      "Epoch 38/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 179342560.0000 - val_loss: 168014176.0000\n",
      "Epoch 39/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 168430144.0000 - val_loss: 157718224.0000\n",
      "Epoch 40/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 157850000.0000 - val_loss: 147744576.0000\n",
      "Epoch 41/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 147618352.0000 - val_loss: 138107920.0000\n",
      "Epoch 42/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 137749536.0000 - val_loss: 128820832.0000\n",
      "Epoch 43/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 128255704.0000 - val_loss: 119894240.0000\n",
      "Epoch 44/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 119147104.0000 - val_loss: 111336904.0000\n",
      "Epoch 45/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 110431920.0000 - val_loss: 103155840.0000\n",
      "Epoch 46/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 102116328.0000 - val_loss: 95356056.0000\n",
      "Epoch 47/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 94204480.0000 - val_loss: 87940688.0000\n",
      "Epoch 48/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 86698528.0000 - val_loss: 80911080.0000\n",
      "Epoch 49/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 79598680.0000 - val_loss: 74266664.0000\n",
      "Epoch 50/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 72903360.0000 - val_loss: 68005216.0000\n",
      "Epoch 51/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 66609012.0000 - val_loss: 62122716.0000\n",
      "Epoch 52/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 60710448.0000 - val_loss: 56613580.0000\n",
      "Epoch 53/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 55200736.0000 - val_loss: 51470656.0000\n",
      "Epoch 54/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 50071396.0000 - val_loss: 46685348.0000\n",
      "Epoch 55/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 45312508.0000 - val_loss: 42247780.0000\n",
      "Epoch 56/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 40912808.0000 - val_loss: 38146824.0000\n",
      "Epoch 57/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 36859784.0000 - val_loss: 34370272.0000\n",
      "Epoch 58/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 33139880.0000 - val_loss: 30904888.0000\n",
      "Epoch 59/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 29738598.0000 - val_loss: 27736758.0000\n",
      "Epoch 60/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 26640666.0000 - val_loss: 24851174.0000\n",
      "Epoch 61/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 23830208.0000 - val_loss: 22232984.0000\n",
      "Epoch 62/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 21290866.0000 - val_loss: 19866618.0000\n",
      "Epoch 63/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 19005978.0000 - val_loss: 17736302.0000\n",
      "Epoch 64/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 16958730.0000 - val_loss: 15826148.0000\n",
      "Epoch 65/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 15132328.0000 - val_loss: 14120388.0000\n",
      "Epoch 66/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 13510107.0000 - val_loss: 12603393.0000\n",
      "Epoch 67/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 12075689.0000 - val_loss: 11259850.0000\n",
      "Epoch 68/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 10813116.0000 - val_loss: 10074926.0000\n",
      "Epoch 69/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 9706939.0000 - val_loss: 9034265.0000\n",
      "Epoch 70/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 8742337.0000 - val_loss: 8124138.0000\n",
      "Epoch 71/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 11ms/step - loss: 7905194.0000 - val_loss: 7331535.5000\n",
      "Epoch 72/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 7182179.0000 - val_loss: 6644168.0000\n",
      "Epoch 73/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 6560773.0000 - val_loss: 6050542.5000\n",
      "Epoch 74/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 6029334.0000 - val_loss: 5539987.0000\n",
      "Epoch 75/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 5577099.0000 - val_loss: 5102663.5000\n",
      "Epoch 76/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 5194193.5000 - val_loss: 4729551.5000\n",
      "Epoch 77/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4871626.0000 - val_loss: 4412458.0000\n",
      "Epoch 78/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4601269.0000 - val_loss: 4143990.0000\n",
      "Epoch 79/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4375835.0000 - val_loss: 3917510.0000\n",
      "Epoch 80/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 4188825.2500 - val_loss: 3727120.5000\n",
      "Epoch 81/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 4034490.2500 - val_loss: 3567589.2500\n",
      "Epoch 82/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3907780.0000 - val_loss: 3434328.7500\n",
      "Epoch 83/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3804289.7500 - val_loss: 3323326.2500\n",
      "Epoch 84/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3720203.0000 - val_loss: 3231105.0000\n",
      "Epoch 85/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3652236.5000 - val_loss: 3154658.5000\n",
      "Epoch 86/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3597586.0000 - val_loss: 3091421.7500\n",
      "Epoch 87/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3553870.2500 - val_loss: 3039194.7500\n",
      "Epoch 88/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3519082.0000 - val_loss: 2996118.0000\n",
      "Epoch 89/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3491541.2500 - val_loss: 2960624.5000\n",
      "Epoch 90/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3469850.2500 - val_loss: 2931399.0000\n",
      "Epoch 91/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3452853.0000 - val_loss: 2907337.7500\n",
      "Epoch 92/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 3439601.5000 - val_loss: 2887530.5000\n",
      "Epoch 93/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3429321.0000 - val_loss: 2871215.5000\n",
      "Epoch 94/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3421384.5000 - val_loss: 2857771.7500\n",
      "Epoch 95/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3415287.7500 - val_loss: 2846682.7500\n",
      "Epoch 96/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3410626.0000 - val_loss: 2837525.2500\n",
      "Epoch 97/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3407076.2500 - val_loss: 2829954.0000\n",
      "Epoch 98/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 3404384.7500 - val_loss: 2823685.0000\n",
      "Epoch 99/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3402353.5000 - val_loss: 2818488.0000\n",
      "Epoch 100/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3400825.0000 - val_loss: 2814175.0000\n",
      "Epoch 101/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3399677.5000 - val_loss: 2810591.0000\n",
      "Epoch 102/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 3398818.7500 - val_loss: 2807608.2500\n",
      "Epoch 103/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3398175.5000 - val_loss: 2805124.5000\n",
      "Epoch 104/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 3397691.2500 - val_loss: 2803048.2500\n",
      "Epoch 105/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 3397296.2500 - val_loss: 2801156.7500\n",
      "Epoch 106/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3354279.7500 - val_loss: 2740023.2500\n",
      "Epoch 107/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 3181789.7500 - val_loss: 2507449.5000\n",
      "Epoch 108/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 2643383.5000 - val_loss: 2081123.0000\n",
      "Epoch 109/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 2161996.0000 - val_loss: 1717796.5000\n",
      "Epoch 110/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 1816478.1250 - val_loss: 1432099.0000\n",
      "Epoch 111/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 1679787.8750 - val_loss: 1262997.8750\n",
      "Epoch 112/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 1695040.7500 - val_loss: 1181006.1250\n",
      "Epoch 113/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 1317645.3750 - val_loss: 1038188.0000\n",
      "Epoch 114/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 1322767.8750 - val_loss: 1068543.8750\n",
      "Epoch 115/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 1122448.8750 - val_loss: 888085.7500\n",
      "Epoch 116/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 1022422.6875 - val_loss: 811368.2500\n",
      "Epoch 117/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 903125.0625 - val_loss: 709519.5625\n",
      "Epoch 118/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 809033.8750 - val_loss: 632809.6875\n",
      "Epoch 119/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 724208.2500 - val_loss: 563331.0625\n",
      "Epoch 120/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 649940.0625 - val_loss: 503515.1562\n",
      "Epoch 121/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 585202.5000 - val_loss: 451611.3438\n",
      "Epoch 122/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 528764.4375 - val_loss: 406432.2812\n",
      "Epoch 123/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 479548.8750 - val_loss: 366974.4375\n",
      "Epoch 124/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 436509.3438 - val_loss: 332358.0938\n",
      "Epoch 125/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 398723.1562 - val_loss: 301863.0938\n",
      "Epoch 126/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 365405.3438 - val_loss: 274895.5312\n",
      "Epoch 127/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 335900.0000 - val_loss: 250966.8906\n",
      "Epoch 128/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 309663.9062 - val_loss: 229667.7031\n",
      "Epoch 129/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 286245.2500 - val_loss: 210656.3438\n",
      "Epoch 130/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 265269.5625 - val_loss: 193642.2812\n",
      "Epoch 131/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 246422.2969 - val_loss: 178379.9688\n",
      "Epoch 132/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 229439.1406 - val_loss: 164660.0156\n",
      "Epoch 133/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 214097.2031 - val_loss: 152304.1875\n",
      "Epoch 134/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 200205.3750 - val_loss: 141159.7812\n",
      "Epoch 135/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 187599.2500 - val_loss: 131098.1250\n",
      "Epoch 136/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 176137.2031 - val_loss: 122007.0938\n",
      "Epoch 137/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 165695.6406 - val_loss: 113793.7031\n",
      "Epoch 138/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 156166.1562 - val_loss: 106374.9141\n",
      "Epoch 139/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 147454.4219 - val_loss: 99681.8438\n",
      "Epoch 140/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 139476.7500 - val_loss: 93651.5156\n",
      "Epoch 141/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 132158.8125 - val_loss: 88230.6562\n",
      "Epoch 142/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 125432.2734 - val_loss: 83370.1719\n",
      "Epoch 143/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 119234.1094 - val_loss: 79027.1172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 113504.1719 - val_loss: 75160.2422\n",
      "Epoch 145/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 108185.9922 - val_loss: 71730.3125\n",
      "Epoch 146/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 103225.1719 - val_loss: 68691.8750\n",
      "Epoch 147/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 98571.0156 - val_loss: 65992.9453\n",
      "Epoch 148/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 94176.4453 - val_loss: 63567.9922\n",
      "Epoch 149/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 89999.1641 - val_loss: 61339.9570\n",
      "Epoch 150/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 86004.2031 - val_loss: 59223.2070\n",
      "Epoch 151/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 82164.5234 - val_loss: 57135.7773\n",
      "Epoch 152/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 78465.5859 - val_loss: 55017.7188\n",
      "Epoch 153/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 74904.9766 - val_loss: 52843.1523\n",
      "Epoch 154/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 71489.7266 - val_loss: 50627.1328\n",
      "Epoch 155/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 68230.7109 - val_loss: 48411.8438\n",
      "Epoch 156/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 65135.2539 - val_loss: 46249.2031\n",
      "Epoch 157/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 62206.2695 - val_loss: 44183.3711\n",
      "Epoch 158/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 59442.2852 - val_loss: 42241.3633\n",
      "Epoch 159/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 56839.1523 - val_loss: 40433.6836\n",
      "Epoch 160/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 54391.3516 - val_loss: 38757.7070\n",
      "Epoch 161/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 52091.4961 - val_loss: 37201.6562\n",
      "Epoch 162/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 49930.7773 - val_loss: 35749.8789\n",
      "Epoch 163/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 47899.5469 - val_loss: 34383.9844\n",
      "Epoch 164/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 45987.9180 - val_loss: 33086.0000\n",
      "Epoch 165/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 44186.1992 - val_loss: 31837.4570\n",
      "Epoch 166/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 42485.5781 - val_loss: 30621.4238\n",
      "Epoch 167/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 40877.8867 - val_loss: 29421.5078\n",
      "Epoch 168/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 39355.3750 - val_loss: 28222.7266\n",
      "Epoch 169/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 37910.5938 - val_loss: 27013.8906\n",
      "Epoch 170/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 36536.4023 - val_loss: 25788.0684\n",
      "Epoch 171/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 35225.0938 - val_loss: 24545.1934\n",
      "Epoch 172/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 33968.8984 - val_loss: 23293.8379\n",
      "Epoch 173/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 32760.1055 - val_loss: 22051.5371\n",
      "Epoch 174/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 31590.6035 - val_loss: 20844.0586\n",
      "Epoch 175/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 30453.2500 - val_loss: 19700.8145\n",
      "Epoch 176/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 29341.5879 - val_loss: 18648.7402\n",
      "Epoch 177/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 28251.3574 - val_loss: 17705.7285\n",
      "Epoch 178/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 27180.1172 - val_loss: 16875.3223\n",
      "Epoch 179/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 26128.0488 - val_loss: 16145.4258\n",
      "Epoch 180/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 25097.9180 - val_loss: 15493.9072\n",
      "Epoch 181/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 24094.3652 - val_loss: 14896.1104\n",
      "Epoch 182/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 23123.2441 - val_loss: 14334.0645\n",
      "Epoch 183/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 22190.6973 - val_loss: 13799.0703\n",
      "Epoch 184/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 21301.4238 - val_loss: 13289.3652\n",
      "Epoch 185/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 20458.0156 - val_loss: 12805.8818\n",
      "Epoch 186/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 19660.5527 - val_loss: 12348.1318\n",
      "Epoch 187/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 18907.1719 - val_loss: 11914.9961\n",
      "Epoch 188/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 18195.0996 - val_loss: 11503.9854\n",
      "Epoch 189/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 17521.4648 - val_loss: 11112.7607\n",
      "Epoch 190/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 16883.5254 - val_loss: 10739.0762\n",
      "Epoch 191/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 16279.1240 - val_loss: 10381.0771\n",
      "Epoch 192/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 15706.2881 - val_loss: 10037.5059\n",
      "Epoch 193/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 15163.5547 - val_loss: 9708.1602\n",
      "Epoch 194/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 14649.8965 - val_loss: 9393.2500\n",
      "Epoch 195/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 14164.5879 - val_loss: 9094.2510\n",
      "Epoch 196/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 13707.2090 - val_loss: 8813.9238\n",
      "Epoch 197/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 13277.6777 - val_loss: 8556.0195\n",
      "Epoch 198/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 12876.0264 - val_loss: 8324.8750\n",
      "Epoch 199/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 12502.3633 - val_loss: 8124.1772\n",
      "Epoch 200/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 12156.1201 - val_loss: 7955.2520\n",
      "Epoch 201/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 11835.3115 - val_loss: 7815.0298\n",
      "Epoch 202/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 11535.9863 - val_loss: 7695.6499\n",
      "Epoch 203/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 11251.6592 - val_loss: 7586.0562\n",
      "Epoch 204/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 10974.4980 - val_loss: 7475.9707\n",
      "Epoch 205/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 10696.5811 - val_loss: 7359.4746\n",
      "Epoch 206/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 10412.3203 - val_loss: 7237.7358\n",
      "Epoch 207/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 10120.0645 - val_loss: 7117.1631\n",
      "Epoch 208/700\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 9822.1514 - val_loss: 7006.5005\n",
      "Epoch 209/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 9524.3477 - val_loss: 6912.5522\n",
      "Epoch 210/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 9233.9541 - val_loss: 6837.3745\n",
      "Epoch 211/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 8957.7373 - val_loss: 6778.4028\n",
      "Epoch 212/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 8700.4600 - val_loss: 6729.9463\n",
      "Epoch 213/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 8464.2900 - val_loss: 6686.1992\n",
      "Epoch 214/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 8248.9492 - val_loss: 6643.0078\n",
      "Epoch 215/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 8052.9204 - val_loss: 6598.0142\n",
      "Epoch 216/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7874.3970 - val_loss: 6550.4683\n",
      "Epoch 217/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7711.7812 - val_loss: 6500.4331\n",
      "Epoch 218/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7563.8647 - val_loss: 6448.3130\n",
      "Epoch 219/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 9ms/step - loss: 7429.7651 - val_loss: 6395.0825\n",
      "Epoch 220/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7309.0005 - val_loss: 6341.5029\n",
      "Epoch 221/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7201.3794 - val_loss: 6288.6475\n",
      "Epoch 222/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7106.8647 - val_loss: 6237.1206\n",
      "Epoch 223/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 7025.6426 - val_loss: 6187.9580\n",
      "Epoch 224/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6957.7002 - val_loss: 6141.8472\n",
      "Epoch 225/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6903.1714 - val_loss: 6099.5781\n",
      "Epoch 226/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6861.4717 - val_loss: 6061.6562\n",
      "Epoch 227/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6831.4570 - val_loss: 6028.2783\n",
      "Epoch 228/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 6811.1104 - val_loss: 5998.4937\n",
      "Epoch 229/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6797.2510 - val_loss: 5970.3384\n",
      "Epoch 230/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6785.5200 - val_loss: 5940.9692\n",
      "Epoch 231/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6771.5005 - val_loss: 5907.0171\n",
      "Epoch 232/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6751.0103 - val_loss: 5865.8589\n",
      "Epoch 233/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6721.3350 - val_loss: 5816.6118\n",
      "Epoch 234/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6681.3486 - val_loss: 5759.3794\n",
      "Epoch 235/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6631.7637 - val_loss: 5695.6489\n",
      "Epoch 236/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6574.5723 - val_loss: 5627.0234\n",
      "Epoch 237/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6511.9048 - val_loss: 5555.4546\n",
      "Epoch 238/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6446.0635 - val_loss: 5481.9712\n",
      "Epoch 239/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6378.9741 - val_loss: 5407.5371\n",
      "Epoch 240/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6311.8281 - val_loss: 5332.5767\n",
      "Epoch 241/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6245.5249 - val_loss: 5257.5894\n",
      "Epoch 242/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6180.5020 - val_loss: 5182.5352\n",
      "Epoch 243/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6116.8550 - val_loss: 5107.7036\n",
      "Epoch 244/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 6054.7856 - val_loss: 5033.1821\n",
      "Epoch 245/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5994.1992 - val_loss: 4959.0049\n",
      "Epoch 246/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5934.9785 - val_loss: 4885.4180\n",
      "Epoch 247/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 5877.0771 - val_loss: 4812.4639\n",
      "Epoch 248/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5820.4355 - val_loss: 4740.4536\n",
      "Epoch 249/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5764.9775 - val_loss: 4669.3950\n",
      "Epoch 250/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5710.6611 - val_loss: 4599.4482\n",
      "Epoch 251/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5657.3330 - val_loss: 4530.8325\n",
      "Epoch 252/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 5605.0562 - val_loss: 4463.6748\n",
      "Epoch 253/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5553.9233 - val_loss: 4397.9639\n",
      "Epoch 254/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5503.6655 - val_loss: 4333.9146\n",
      "Epoch 255/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5454.3408 - val_loss: 4271.5342\n",
      "Epoch 256/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5406.0137 - val_loss: 4210.9575\n",
      "Epoch 257/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5358.5337 - val_loss: 4152.0835\n",
      "Epoch 258/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 5311.9087 - val_loss: 4095.0867\n",
      "Epoch 259/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 5266.2773 - val_loss: 4039.8052\n",
      "Epoch 260/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5221.3252 - val_loss: 3986.3940\n",
      "Epoch 261/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5177.0547 - val_loss: 3934.8569\n",
      "Epoch 262/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5133.5908 - val_loss: 3885.0481\n",
      "Epoch 263/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5090.8291 - val_loss: 3837.0344\n",
      "Epoch 264/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5048.7539 - val_loss: 3790.7881\n",
      "Epoch 265/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 5007.4023 - val_loss: 3746.2241\n",
      "Epoch 266/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4966.6411 - val_loss: 3703.3943\n",
      "Epoch 267/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4926.2632 - val_loss: 3662.2312\n",
      "Epoch 268/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4886.7158 - val_loss: 3622.7051\n",
      "Epoch 269/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4847.6489 - val_loss: 3584.8345\n",
      "Epoch 270/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 4809.1841 - val_loss: 3548.3511\n",
      "Epoch 271/700\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 4771.1729 - val_loss: 3513.4514\n",
      "Epoch 272/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4733.6826 - val_loss: 3479.9058\n",
      "Epoch 273/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4696.6084 - val_loss: 3447.8159\n",
      "Epoch 274/700\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 4660.0498 - val_loss: 3417.0515\n",
      "Epoch 275/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4623.9771 - val_loss: 3387.6279\n",
      "Epoch 276/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4588.3965 - val_loss: 3359.2866\n",
      "Epoch 277/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4553.1953 - val_loss: 3332.2581\n",
      "Epoch 278/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4518.4399 - val_loss: 3306.3774\n",
      "Epoch 279/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4484.2144 - val_loss: 3281.4409\n",
      "Epoch 280/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4450.2988 - val_loss: 3257.5872\n",
      "Epoch 281/700\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 4416.8159 - val_loss: 3234.7183\n",
      "Epoch 282/700\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 4383.7124 - val_loss: 3212.7988\n",
      "Epoch 283/700\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 3557.5376"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40, min_delta=0.001)\n",
    "history_tech=model_tech.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=700,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[es]\n",
    ")\n",
    "y_pred = model_tech.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_tech.history['loss'], label='train')\n",
    "plt.plot(history_tech.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(y_train)), y_train, 'g', label=\"history\")\n",
    "plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test, marker='.', label=\"true\")\n",
    "plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred, 'r', label=\"prediction\")\n",
    "# plt.plot(np.arange(0, len(y_train)), sc_y.inverse_transform(y_train), 'g', label=\"history\")\n",
    "# plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), sc_y.inverse_transform(y_test), marker='.', label=\"true\")\n",
    "# plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), sc_y.inverse_transform(y_pred), 'r', label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134553b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_test = sc.inverse_transform(y_test)\n",
    "# y_pred = sc.inverse_transform(y_pred)\n",
    "plt.plot(y_test, marker='.', label=\"true\")\n",
    "plt.plot(y_pred, 'r', label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf212a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,y_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=data_val.drop('Prediction',1).values\n",
    "y_val=data_val['Prediction']\n",
    "# x_val=x_val.reshape(x_val.shape[0],1,x_val.shape[1])\n",
    "x_val=sc.transform(x_val).reshape(x_val.shape[0],1,x_val.shape[1])\n",
    "y_val_pred=model_tech.predict(x_val)\n",
    "# y_val_pred=sc_y.inverse_transform(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_val.values, marker='.', label=\"true\")\n",
    "plt.plot(y_val_pred, 'r', marker='*',label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_val.values,y_val_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab65e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
