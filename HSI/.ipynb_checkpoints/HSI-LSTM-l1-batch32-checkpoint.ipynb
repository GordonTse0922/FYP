{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9ae2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score, mean_squared_error,mean_absolute_percentage_error,r2_score\n",
    "import tensorflow as tf\n",
    "import talib\n",
    "from tensorflow import keras\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad5f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_26933/3609140658.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data.drop('Volume',1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "      <td>30515.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "      <td>30560.900391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "      <td>30736.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "      <td>30814.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "      <td>30899.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "      <td>23397.699219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "Date                                                                 \n",
       "2018-01-02  30515.300781  30515.300781  30515.300781  30515.300781   \n",
       "2018-01-03  30560.900391  30560.900391  30560.900391  30560.900391   \n",
       "2018-01-04  30736.500000  30736.500000  30736.500000  30736.500000   \n",
       "2018-01-05  30814.599609  30814.599609  30814.599609  30814.599609   \n",
       "2018-01-08  30899.500000  30899.500000  30899.500000  30899.500000   \n",
       "...                  ...           ...           ...           ...   \n",
       "2021-12-27  23223.800781  23223.800781  23223.800781  23223.800781   \n",
       "2021-12-28  23280.599609  23280.599609  23280.599609  23280.599609   \n",
       "2021-12-29  23086.500000  23086.500000  23086.500000  23086.500000   \n",
       "2021-12-30  23112.000000  23112.000000  23112.000000  23112.000000   \n",
       "2021-12-31  23397.699219  23397.699219  23397.699219  23397.699219   \n",
       "\n",
       "               Adj Close  \n",
       "Date                      \n",
       "2018-01-02  30515.300781  \n",
       "2018-01-03  30560.900391  \n",
       "2018-01-04  30736.500000  \n",
       "2018-01-05  30814.599609  \n",
       "2018-01-08  30899.500000  \n",
       "...                  ...  \n",
       "2021-12-27  23223.800781  \n",
       "2021-12-28  23280.599609  \n",
       "2021-12-29  23086.500000  \n",
       "2021-12-30  23112.000000  \n",
       "2021-12-31  23397.699219  \n",
       "\n",
       "[822 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=yf.download('HSI','2018-01-01','2022-01-01')\n",
    "data.drop('Volume',1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781ab9b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['6day MA'] = data['Close'].rolling(window = 6).mean()\n",
    "data['12day MA'] = data['Close'].rolling(window = 12).mean()\n",
    "data['RSI'] = talib.RSI(data['Close'].values, timeperiod = 7)\n",
    "data['%R5'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 5)\n",
    "data['%R10'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 10)\n",
    "data['MI6']=talib.MOM(data['Close'],timeperiod=6)\n",
    "data['MI12']=talib.MOM(data['Close'],timeperiod=12)\n",
    "macd, macdsignal, macdhist = talib.MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "data['EMA12']=talib.EMA(data['Close'], timeperiod=12)\n",
    "data['EMA26']=talib.EMA(data['Close'],timeperiod=26)\n",
    "data['MACD']=macd\n",
    "data['TR']=talib.TRANGE(data['High'],data['Low'],data['Close'])\n",
    "data['OSC6']=talib.CMO(data['Close'], timeperiod=6)\n",
    "data['OSC12']=talib.CMO(data['Close'], timeperiod=12)\n",
    "data['Prediction']=data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68726853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_26933/1968958741.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  importance=mutual_info_regression(data.drop('Prediction',1),data['Prediction'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "importance=mutual_info_regression(data.drop('Prediction',1),data['Prediction'])\n",
    "feature_importances=pd.Series(importance,data.columns[0:len(data.columns)-1])\n",
    "feature_importances.plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1036fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_26933/244261082.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data.drop(['OSC12','OSC6','MI12','MI6','%R10','%R5','RSI','TR'],1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>6day MA</th>\n",
       "      <th>12day MA</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-12-23</th>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23193.599609</td>\n",
       "      <td>23113.366862</td>\n",
       "      <td>23494.916829</td>\n",
       "      <td>23397.249467</td>\n",
       "      <td>23841.259170</td>\n",
       "      <td>-444.009703</td>\n",
       "      <td>23223.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23223.800781</td>\n",
       "      <td>23071.416992</td>\n",
       "      <td>23430.491862</td>\n",
       "      <td>23370.565054</td>\n",
       "      <td>23795.521511</td>\n",
       "      <td>-424.956457</td>\n",
       "      <td>23280.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23280.599609</td>\n",
       "      <td>23086.083659</td>\n",
       "      <td>23349.300130</td>\n",
       "      <td>23356.724216</td>\n",
       "      <td>23757.379148</td>\n",
       "      <td>-400.654932</td>\n",
       "      <td>23086.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23086.500000</td>\n",
       "      <td>23143.016927</td>\n",
       "      <td>23273.533529</td>\n",
       "      <td>23315.151260</td>\n",
       "      <td>23707.684396</td>\n",
       "      <td>-392.533137</td>\n",
       "      <td>23112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23112.000000</td>\n",
       "      <td>23166.466797</td>\n",
       "      <td>23203.316895</td>\n",
       "      <td>23283.897220</td>\n",
       "      <td>23663.559626</td>\n",
       "      <td>-379.662406</td>\n",
       "      <td>23397.699219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "Date                                                                 \n",
       "2021-12-23  23193.599609  23193.599609  23193.599609  23193.599609   \n",
       "2021-12-27  23223.800781  23223.800781  23223.800781  23223.800781   \n",
       "2021-12-28  23280.599609  23280.599609  23280.599609  23280.599609   \n",
       "2021-12-29  23086.500000  23086.500000  23086.500000  23086.500000   \n",
       "2021-12-30  23112.000000  23112.000000  23112.000000  23112.000000   \n",
       "\n",
       "               Adj Close       6day MA      12day MA         EMA12  \\\n",
       "Date                                                                 \n",
       "2021-12-23  23193.599609  23113.366862  23494.916829  23397.249467   \n",
       "2021-12-27  23223.800781  23071.416992  23430.491862  23370.565054   \n",
       "2021-12-28  23280.599609  23086.083659  23349.300130  23356.724216   \n",
       "2021-12-29  23086.500000  23143.016927  23273.533529  23315.151260   \n",
       "2021-12-30  23112.000000  23166.466797  23203.316895  23283.897220   \n",
       "\n",
       "                   EMA26        MACD    Prediction  \n",
       "Date                                                \n",
       "2021-12-23  23841.259170 -444.009703  23223.800781  \n",
       "2021-12-27  23795.521511 -424.956457  23280.599609  \n",
       "2021-12-28  23757.379148 -400.654932  23086.500000  \n",
       "2021-12-29  23707.684396 -392.533137  23112.000000  \n",
       "2021-12-30  23663.559626 -379.662406  23397.699219  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['OSC12','OSC6','MI12','MI6','%R10','%R5','RSI','TR'],1,inplace=True)\n",
    "data_val=data[-100:]\n",
    "data=data[:-100]\n",
    "data_val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b49c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "sc_y= MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64aa428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Prediction']=data['Close'].shift(-1)\n",
    "# data.dropna(inplace=True)\n",
    "# data_val=data[-100:]\n",
    "# data=data[:-100]\n",
    "# data_val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb6680b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619, 1, 10) (619,) (69, 1, 10) (69,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_26933/2884921294.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X=sc.fit_transform(data.drop('Prediction',1))\n"
     ]
    }
   ],
   "source": [
    "X=sc.fit_transform(data.drop('Prediction',1))\n",
    "#X=data.drop('Prediction',1).values\n",
    "# Y = data[\"Prediction\"].values.reshape(-1,1)\n",
    "# Y = sc_y.fit_transform(data[\"Prediction\"].values.reshape(-1,1))\n",
    "Y=np.array(data[\"Close\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8c5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:37:14.189383: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-10 00:37:14.189508: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model_tech = keras.Sequential()\n",
    "model_tech.add(keras.layers.LSTM(\n",
    "  units=128,\n",
    "  input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    return_sequences=True,\n",
    "    kernel_regularizer=regularizers.l1(0.01)\n",
    "   \n",
    "))\n",
    "model_tech.add(keras.layers.LSTM(units=64))\n",
    "model_tech.add(keras.layers.Dense(units=64))\n",
    "model_tech.add(keras.layers.Dense(units=1))\n",
    "model_tech.compile(\n",
    "  loss='mean_squared_error',\n",
    "  optimizer=keras.optimizers.Adam(0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d66a4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:37:14.716660: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-10 00:37:15.997798: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:37:16.300950: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:37:16.492824: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:37:17.133663: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:37:17.312959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 544385600.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:37:18.675993: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:37:18.752735: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-10 00:37:18.772733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 78ms/step - loss: 544385600.0000 - val_loss: 513893856.0000\n",
      "Epoch 2/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 544238592.0000 - val_loss: 513563392.0000\n",
      "Epoch 3/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 543545152.0000 - val_loss: 512498560.0000\n",
      "Epoch 4/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 542177536.0000 - val_loss: 511089920.0000\n",
      "Epoch 5/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 540663360.0000 - val_loss: 509670816.0000\n",
      "Epoch 6/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 539100992.0000 - val_loss: 508153056.0000\n",
      "Epoch 7/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 537406528.0000 - val_loss: 506490752.0000\n",
      "Epoch 8/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 535549600.0000 - val_loss: 504669728.0000\n",
      "Epoch 9/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 533519296.0000 - val_loss: 502683840.0000\n",
      "Epoch 10/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 531310496.0000 - val_loss: 500530208.0000\n",
      "Epoch 11/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 528920960.0000 - val_loss: 498207488.0000\n",
      "Epoch 12/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 526349856.0000 - val_loss: 495715616.0000\n",
      "Epoch 13/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 523596992.0000 - val_loss: 493054656.0000\n",
      "Epoch 14/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 520663072.0000 - val_loss: 490225344.0000\n",
      "Epoch 15/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 517548576.0000 - val_loss: 487228256.0000\n",
      "Epoch 16/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 514254944.0000 - val_loss: 484064672.0000\n",
      "Epoch 17/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 510784064.0000 - val_loss: 480736480.0000\n",
      "Epoch 18/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 507138592.0000 - val_loss: 477246208.0000\n",
      "Epoch 19/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 503322016.0000 - val_loss: 473597344.0000\n",
      "Epoch 20/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 499338048.0000 - val_loss: 469793152.0000\n",
      "Epoch 21/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 495190720.0000 - val_loss: 465836768.0000\n",
      "Epoch 22/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 490884128.0000 - val_loss: 461731040.0000\n",
      "Epoch 23/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 486422144.0000 - val_loss: 457478464.0000\n",
      "Epoch 24/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 481809120.0000 - val_loss: 453081856.0000\n",
      "Epoch 25/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 477049248.0000 - val_loss: 448545152.0000\n",
      "Epoch 26/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 472147360.0000 - val_loss: 443874048.0000\n",
      "Epoch 27/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 467108448.0000 - val_loss: 439076800.0000\n",
      "Epoch 28/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 461938080.0000 - val_loss: 434161600.0000\n",
      "Epoch 29/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 456641344.0000 - val_loss: 429134528.0000\n",
      "Epoch 30/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 451223712.0000 - val_loss: 423999744.0000\n",
      "Epoch 31/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 445690176.0000 - val_loss: 418760704.0000\n",
      "Epoch 32/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 440045824.0000 - val_loss: 413421312.0000\n",
      "Epoch 33/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 434295936.0000 - val_loss: 407985920.0000\n",
      "Epoch 34/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 428445632.0000 - val_loss: 402459328.0000\n",
      "Epoch 35/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 422500576.0000 - val_loss: 396846240.0000\n",
      "Epoch 36/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 416466016.0000 - val_loss: 391151872.0000\n",
      "Epoch 37/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 410347456.0000 - val_loss: 385381056.0000\n",
      "Epoch 38/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 404150336.0000 - val_loss: 379539104.0000\n",
      "Epoch 39/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 397880224.0000 - val_loss: 373631104.0000\n",
      "Epoch 40/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 391542688.0000 - val_loss: 367662112.0000\n",
      "Epoch 41/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 385143104.0000 - val_loss: 361637280.0000\n",
      "Epoch 42/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 378687104.0000 - val_loss: 355561760.0000\n",
      "Epoch 43/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 372180128.0000 - val_loss: 349440640.0000\n",
      "Epoch 44/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 365627712.0000 - val_loss: 343279040.0000\n",
      "Epoch 45/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 359035200.0000 - val_loss: 337082016.0000\n",
      "Epoch 46/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 352408128.0000 - val_loss: 330854528.0000\n",
      "Epoch 47/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 345751712.0000 - val_loss: 324601568.0000\n",
      "Epoch 48/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 339071456.0000 - val_loss: 318328192.0000\n",
      "Epoch 49/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 332372512.0000 - val_loss: 312039232.0000\n",
      "Epoch 50/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 325660224.0000 - val_loss: 305739488.0000\n",
      "Epoch 51/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 318939648.0000 - val_loss: 299433792.0000\n",
      "Epoch 52/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 312215904.0000 - val_loss: 293126848.0000\n",
      "Epoch 53/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 305494144.0000 - val_loss: 286823360.0000\n",
      "Epoch 54/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 298779200.0000 - val_loss: 280527840.0000\n",
      "Epoch 55/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 292075936.0000 - val_loss: 274244864.0000\n",
      "Epoch 56/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 285389184.0000 - val_loss: 267978816.0000\n",
      "Epoch 57/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 278723712.0000 - val_loss: 261734064.0000\n",
      "Epoch 58/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 272084096.0000 - val_loss: 255514880.0000\n",
      "Epoch 59/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 265474816.0000 - val_loss: 249325440.0000\n",
      "Epoch 60/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 258900320.0000 - val_loss: 243169808.0000\n",
      "Epoch 61/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 252365024.0000 - val_loss: 237051984.0000\n",
      "Epoch 62/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 245872992.0000 - val_loss: 230975840.0000\n",
      "Epoch 63/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 239428464.0000 - val_loss: 224945216.0000\n",
      "Epoch 64/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 233035360.0000 - val_loss: 218963728.0000\n",
      "Epoch 65/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 226697616.0000 - val_loss: 213035008.0000\n",
      "Epoch 66/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 220418960.0000 - val_loss: 207162432.0000\n",
      "Epoch 67/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 214203040.0000 - val_loss: 201349408.0000\n",
      "Epoch 68/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 208053408.0000 - val_loss: 195599152.0000\n",
      "Epoch 69/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 201973456.0000 - val_loss: 189914784.0000\n",
      "Epoch 70/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 195966400.0000 - val_loss: 184299328.0000\n",
      "Epoch 71/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 10ms/step - loss: 190035440.0000 - val_loss: 178755648.0000\n",
      "Epoch 72/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 184183520.0000 - val_loss: 173286432.0000\n",
      "Epoch 73/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 178413552.0000 - val_loss: 167894336.0000\n",
      "Epoch 74/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 172728288.0000 - val_loss: 162581920.0000\n",
      "Epoch 75/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 167130224.0000 - val_loss: 157351472.0000\n",
      "Epoch 76/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 161621920.0000 - val_loss: 152205248.0000\n",
      "Epoch 77/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 156205616.0000 - val_loss: 147145360.0000\n",
      "Epoch 78/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 150883488.0000 - val_loss: 142173776.0000\n",
      "Epoch 79/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 145657584.0000 - val_loss: 137292336.0000\n",
      "Epoch 80/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 140529760.0000 - val_loss: 132502704.0000\n",
      "Epoch 81/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 135501776.0000 - val_loss: 127806488.0000\n",
      "Epoch 82/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 130575176.0000 - val_loss: 123205128.0000\n",
      "Epoch 83/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 125751440.0000 - val_loss: 118699864.0000\n",
      "Epoch 84/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 121031832.0000 - val_loss: 114291880.0000\n",
      "Epoch 85/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 116417496.0000 - val_loss: 109982256.0000\n",
      "Epoch 86/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 111909464.0000 - val_loss: 105771800.0000\n",
      "Epoch 87/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 107508568.0000 - val_loss: 101661296.0000\n",
      "Epoch 88/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 103215496.0000 - val_loss: 97651360.0000\n",
      "Epoch 89/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 99030856.0000 - val_loss: 93742472.0000\n",
      "Epoch 90/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 94955032.0000 - val_loss: 89934984.0000\n",
      "Epoch 91/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 90988296.0000 - val_loss: 86229120.0000\n",
      "Epoch 92/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 87130824.0000 - val_loss: 82624960.0000\n",
      "Epoch 93/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 83382544.0000 - val_loss: 79122480.0000\n",
      "Epoch 94/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 79743384.0000 - val_loss: 75721512.0000\n",
      "Epoch 95/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 76213056.0000 - val_loss: 72421744.0000\n",
      "Epoch 96/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 72791120.0000 - val_loss: 69222784.0000\n",
      "Epoch 97/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 69477056.0000 - val_loss: 66124096.0000\n",
      "Epoch 98/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 66270200.0000 - val_loss: 63125056.0000\n",
      "Epoch 99/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 63169756.0000 - val_loss: 60224856.0000\n",
      "Epoch 100/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 60174812.0000 - val_loss: 57422640.0000\n",
      "Epoch 101/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 57284352.0000 - val_loss: 54717480.0000\n",
      "Epoch 102/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 54497236.0000 - val_loss: 52108220.0000\n",
      "Epoch 103/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 51812188.0000 - val_loss: 49593716.0000\n",
      "Epoch 104/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 49227872.0000 - val_loss: 47172680.0000\n",
      "Epoch 105/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 46742852.0000 - val_loss: 44843760.0000\n",
      "Epoch 106/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 44355532.0000 - val_loss: 42605468.0000\n",
      "Epoch 107/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 42064292.0000 - val_loss: 40456268.0000\n",
      "Epoch 108/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 39867396.0000 - val_loss: 38394556.0000\n",
      "Epoch 109/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 37763024.0000 - val_loss: 36418640.0000\n",
      "Epoch 110/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 35749316.0000 - val_loss: 34526748.0000\n",
      "Epoch 111/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 33824288.0000 - val_loss: 32717080.0000\n",
      "Epoch 112/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 31985940.0000 - val_loss: 30987718.0000\n",
      "Epoch 113/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 30232174.0000 - val_loss: 29336760.0000\n",
      "Epoch 114/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 28560860.0000 - val_loss: 27762216.0000\n",
      "Epoch 115/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 26969812.0000 - val_loss: 26262042.0000\n",
      "Epoch 116/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 25456806.0000 - val_loss: 24834184.0000\n",
      "Epoch 117/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 24019586.0000 - val_loss: 23476570.0000\n",
      "Epoch 118/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 22655862.0000 - val_loss: 22187054.0000\n",
      "Epoch 119/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21363314.0000 - val_loss: 20963512.0000\n",
      "Epoch 120/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 20139620.0000 - val_loss: 19803788.0000\n",
      "Epoch 121/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 18982428.0000 - val_loss: 18705706.0000\n",
      "Epoch 122/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 17889394.0000 - val_loss: 17667122.0000\n",
      "Epoch 123/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16858166.0000 - val_loss: 16685864.0000\n",
      "Epoch 124/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 15886403.0000 - val_loss: 15759754.0000\n",
      "Epoch 125/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14971758.0000 - val_loss: 14886648.0000\n",
      "Epoch 126/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14111927.0000 - val_loss: 14064430.0000\n",
      "Epoch 127/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 13304603.0000 - val_loss: 13290966.0000\n",
      "Epoch 128/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 12547521.0000 - val_loss: 12564185.0000\n",
      "Epoch 129/700\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 11838428.0000 - val_loss: 11882021.0000\n",
      "Epoch 130/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 11175123.0000 - val_loss: 11242421.0000\n",
      "Epoch 131/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10555430.0000 - val_loss: 10643434.0000\n",
      "Epoch 132/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9977228.0000 - val_loss: 10083090.0000\n",
      "Epoch 133/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 9438434.0000 - val_loss: 9559472.0000\n",
      "Epoch 134/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8937016.0000 - val_loss: 9070738.0000\n",
      "Epoch 135/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8471000.0000 - val_loss: 8615059.0000\n",
      "Epoch 136/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8038451.5000 - val_loss: 8190674.5000\n",
      "Epoch 137/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7637507.5000 - val_loss: 7795880.0000\n",
      "Epoch 138/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 7266361.0000 - val_loss: 7429007.5000\n",
      "Epoch 139/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6923261.0000 - val_loss: 7088460.0000\n",
      "Epoch 140/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 6606521.0000 - val_loss: 6772704.0000\n",
      "Epoch 141/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6314523.0000 - val_loss: 6480236.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6045704.0000 - val_loss: 6209647.0000\n",
      "Epoch 143/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5798574.0000 - val_loss: 5959559.5000\n",
      "Epoch 144/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5571701.5000 - val_loss: 5728668.0000\n",
      "Epoch 145/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5363724.0000 - val_loss: 5515721.0000\n",
      "Epoch 146/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5173337.5000 - val_loss: 5319518.5000\n",
      "Epoch 147/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4999307.0000 - val_loss: 5138942.0000\n",
      "Epoch 148/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4840460.0000 - val_loss: 4972903.5000\n",
      "Epoch 149/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4695684.0000 - val_loss: 4820382.0000\n",
      "Epoch 150/700\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 4563925.0000 - val_loss: 4680420.0000\n",
      "Epoch 151/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4444192.5000 - val_loss: 4552093.5000\n",
      "Epoch 152/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 4335551.5000 - val_loss: 4434543.5000\n",
      "Epoch 153/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4237123.0000 - val_loss: 4326965.0000\n",
      "Epoch 154/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4148084.5000 - val_loss: 4228597.0000\n",
      "Epoch 155/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 4067661.7500 - val_loss: 4138727.0000\n",
      "Epoch 156/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3995138.0000 - val_loss: 4056685.5000\n",
      "Epoch 157/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3929838.2500 - val_loss: 3981850.0000\n",
      "Epoch 158/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3871133.7500 - val_loss: 3913633.2500\n",
      "Epoch 159/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3818447.5000 - val_loss: 3851500.7500\n",
      "Epoch 160/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3771236.5000 - val_loss: 3794943.0000\n",
      "Epoch 161/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3729002.2500 - val_loss: 3743491.5000\n",
      "Epoch 162/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 3691285.0000 - val_loss: 3696714.7500\n",
      "Epoch 163/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3657655.0000 - val_loss: 3654216.7500\n",
      "Epoch 164/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3627724.5000 - val_loss: 3615615.7500\n",
      "Epoch 165/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3601131.7500 - val_loss: 3580577.2500\n",
      "Epoch 166/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3577547.2500 - val_loss: 3548783.7500\n",
      "Epoch 167/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3556667.5000 - val_loss: 3519946.7500\n",
      "Epoch 168/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3538218.2500 - val_loss: 3493796.0000\n",
      "Epoch 169/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3521946.7500 - val_loss: 3470094.0000\n",
      "Epoch 170/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3507624.2500 - val_loss: 3448610.5000\n",
      "Epoch 171/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3495041.5000 - val_loss: 3429143.7500\n",
      "Epoch 172/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3484012.5000 - val_loss: 3411509.0000\n",
      "Epoch 173/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3474363.7500 - val_loss: 3395530.0000\n",
      "Epoch 174/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3465941.5000 - val_loss: 3381058.5000\n",
      "Epoch 175/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3458607.5000 - val_loss: 3367945.7500\n",
      "Epoch 176/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 3452236.0000 - val_loss: 3356068.7500\n",
      "Epoch 177/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3446713.5000 - val_loss: 3345305.7500\n",
      "Epoch 178/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3441940.2500 - val_loss: 3335555.0000\n",
      "Epoch 179/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3437826.2500 - val_loss: 3326717.0000\n",
      "Epoch 180/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3434289.2500 - val_loss: 3318707.0000\n",
      "Epoch 181/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3431259.0000 - val_loss: 3311445.0000\n",
      "Epoch 182/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3428671.0000 - val_loss: 3304855.2500\n",
      "Epoch 183/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3426468.5000 - val_loss: 3298879.5000\n",
      "Epoch 184/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3424601.5000 - val_loss: 3293458.0000\n",
      "Epoch 185/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 3423024.5000 - val_loss: 3288536.0000\n",
      "Epoch 186/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3421699.7500 - val_loss: 3284064.5000\n",
      "Epoch 187/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3420591.2500 - val_loss: 3280007.2500\n",
      "Epoch 188/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3419668.5000 - val_loss: 3276315.7500\n",
      "Epoch 189/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3418907.0000 - val_loss: 3272964.5000\n",
      "Epoch 190/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3418279.7500 - val_loss: 3269914.7500\n",
      "Epoch 191/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 3417769.5000 - val_loss: 3267141.0000\n",
      "Epoch 192/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3417356.2500 - val_loss: 3264614.7500\n",
      "Epoch 193/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3417023.7500 - val_loss: 3262313.0000\n",
      "Epoch 194/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3416732.0000 - val_loss: 3260087.7500\n",
      "Epoch 195/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3413334.7500 - val_loss: 3251735.7500\n",
      "Epoch 196/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3397079.7500 - val_loss: 3220936.7500\n",
      "Epoch 197/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3313120.2500 - val_loss: 3121664.2500\n",
      "Epoch 198/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 3117391.7500 - val_loss: 2930931.5000\n",
      "Epoch 199/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2824635.2500 - val_loss: 2678267.2500\n",
      "Epoch 200/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 2419148.7500 - val_loss: 2341039.5000\n",
      "Epoch 201/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 2091006.5000 - val_loss: 2061376.7500\n",
      "Epoch 202/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1922245.6250 - val_loss: 1872896.6250\n",
      "Epoch 203/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1846361.5000 - val_loss: 1822129.8750\n",
      "Epoch 204/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1714492.7500 - val_loss: 1659718.8750\n",
      "Epoch 205/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 1522600.0000 - val_loss: 1554941.7500\n",
      "Epoch 206/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 1428295.8750 - val_loss: 1478722.1250\n",
      "Epoch 207/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 1356874.6250 - val_loss: 1407655.7500\n",
      "Epoch 208/700\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 1271653.1250 - val_loss: 1322695.6250\n",
      "Epoch 209/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1182526.2500 - val_loss: 1237846.6250\n",
      "Epoch 210/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1104228.7500 - val_loss: 1162814.0000\n",
      "Epoch 211/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1034244.5000 - val_loss: 1093295.6250\n",
      "Epoch 212/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 968751.2500 - val_loss: 1027123.6875\n",
      "Epoch 213/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 907663.6250 - val_loss: 965332.5625\n",
      "Epoch 214/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 10ms/step - loss: 851600.0000 - val_loss: 908256.8125\n",
      "Epoch 215/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 800378.6250 - val_loss: 855516.7500\n",
      "Epoch 216/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 753667.1875 - val_loss: 806848.5625\n",
      "Epoch 217/700\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 711135.5625 - val_loss: 761963.1875\n",
      "Epoch 218/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 672299.4375 - val_loss: 720466.8125\n",
      "Epoch 219/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 636642.9375 - val_loss: 681994.2500\n",
      "Epoch 220/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 603747.8750 - val_loss: 646234.6250\n",
      "Epoch 221/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 573287.2500 - val_loss: 612927.0000\n",
      "Epoch 222/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 545001.6875 - val_loss: 581842.4375\n",
      "Epoch 223/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 518679.3438 - val_loss: 552782.8125\n",
      "Epoch 224/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 494138.0312 - val_loss: 525574.0000\n",
      "Epoch 225/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 471220.6875 - val_loss: 500059.4062\n",
      "Epoch 226/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 449786.3125 - val_loss: 476101.2812\n",
      "Epoch 227/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 429711.4375 - val_loss: 453575.9375\n",
      "Epoch 228/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 410885.5625 - val_loss: 432376.1250\n",
      "Epoch 229/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 393209.0625 - val_loss: 412402.5312\n",
      "Epoch 230/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 376593.9062 - val_loss: 393569.0312\n",
      "Epoch 231/700\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 360959.8750 - val_loss: 375798.1562\n",
      "Epoch 232/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 346235.4375 - val_loss: 359020.1875\n",
      "Epoch 233/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 332355.3438 - val_loss: 343170.3750\n",
      "Epoch 234/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 319260.7188 - val_loss: 328193.5000\n",
      "Epoch 235/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 306898.0000 - val_loss: 314034.7500\n",
      "Epoch 236/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 295217.5938 - val_loss: 300648.0625\n",
      "Epoch 237/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 284175.3438 - val_loss: 287987.0312\n",
      "Epoch 238/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 273727.8750 - val_loss: 276009.7188\n",
      "Epoch 239/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 263837.1875 - val_loss: 264677.9688\n",
      "Epoch 240/700\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 254466.3594 - val_loss: 253952.6406\n",
      "Epoch 241/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 245579.5781 - val_loss: 243800.0000\n",
      "Epoch 242/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 237144.4531 - val_loss: 234184.3438\n",
      "Epoch 243/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 229127.1406 - val_loss: 225070.4219\n",
      "Epoch 244/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 221494.7656 - val_loss: 216424.0938\n",
      "Epoch 245/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 214214.3594 - val_loss: 208209.3750\n",
      "Epoch 246/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 207252.9531 - val_loss: 200386.5000\n",
      "Epoch 247/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 200579.9688 - val_loss: 192919.7656\n",
      "Epoch 248/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 194170.3750 - val_loss: 185772.0156\n",
      "Epoch 249/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 188007.2344 - val_loss: 178914.1562\n",
      "Epoch 250/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 182083.9062 - val_loss: 172333.1719\n",
      "Epoch 251/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 176399.8438 - val_loss: 166025.0625\n",
      "Epoch 252/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 170955.0000 - val_loss: 159992.5938\n",
      "Epoch 253/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 165747.3594 - val_loss: 154238.6406\n",
      "Epoch 254/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 160770.6562 - val_loss: 148756.5000\n",
      "Epoch 255/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 156014.7969 - val_loss: 143538.0000\n",
      "Epoch 256/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 151468.5781 - val_loss: 138571.2344\n",
      "Epoch 257/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 147122.1562 - val_loss: 133846.8906\n",
      "Epoch 258/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 142965.7031 - val_loss: 129354.6719\n",
      "Epoch 259/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 138989.5625 - val_loss: 125085.6719\n",
      "Epoch 260/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 135184.9531 - val_loss: 121030.5781\n",
      "Epoch 261/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 131544.1562 - val_loss: 117179.5312\n",
      "Epoch 262/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 128058.6719 - val_loss: 113521.7734\n",
      "Epoch 263/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 124720.0156 - val_loss: 110043.3672\n",
      "Epoch 264/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 121520.0859 - val_loss: 106730.8672\n",
      "Epoch 265/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 118449.8984 - val_loss: 103566.1406\n",
      "Epoch 266/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 115499.8516 - val_loss: 100531.2734\n",
      "Epoch 267/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 112659.9453 - val_loss: 97605.0859\n",
      "Epoch 268/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 109920.4766 - val_loss: 94768.0469\n",
      "Epoch 269/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 107271.8438 - val_loss: 91999.8203\n",
      "Epoch 270/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 104704.6797 - val_loss: 89281.9531\n",
      "Epoch 271/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 102210.5781 - val_loss: 86597.8359\n",
      "Epoch 272/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 99781.5547 - val_loss: 83935.2578\n",
      "Epoch 273/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 97411.6797 - val_loss: 81285.0938\n",
      "Epoch 274/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 95095.5391 - val_loss: 78643.0938\n",
      "Epoch 275/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 92829.2891 - val_loss: 76009.7266\n",
      "Epoch 276/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 90610.1484 - val_loss: 73390.3750\n",
      "Epoch 277/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 88437.2578 - val_loss: 70794.5156\n",
      "Epoch 278/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 86310.4531 - val_loss: 68235.9141\n",
      "Epoch 279/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 84230.6562 - val_loss: 65730.0234\n",
      "Epoch 280/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 82199.6406 - val_loss: 63292.4102\n",
      "Epoch 281/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 80218.8906 - val_loss: 60937.2383\n",
      "Epoch 282/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 78290.5625 - val_loss: 58676.4180\n",
      "Epoch 283/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 76416.1172 - val_loss: 56516.8438\n",
      "Epoch 284/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 74596.3672 - val_loss: 54461.5625\n",
      "Epoch 285/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 72831.5547 - val_loss: 52508.7305\n",
      "Epoch 286/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 71120.8984 - val_loss: 50654.0312\n",
      "Epoch 287/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 69463.5000 - val_loss: 48890.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 67857.4531 - val_loss: 47211.3203\n",
      "Epoch 289/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 66300.6875 - val_loss: 45607.7812\n",
      "Epoch 290/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 64791.0977 - val_loss: 44073.2109\n",
      "Epoch 291/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 63326.0352 - val_loss: 42601.0078\n",
      "Epoch 292/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 61903.8516 - val_loss: 41186.4414\n",
      "Epoch 293/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 60522.2695 - val_loss: 39824.3867\n",
      "Epoch 294/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 59179.3750 - val_loss: 38511.4844\n",
      "Epoch 295/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 57873.5859 - val_loss: 37244.7891\n",
      "Epoch 296/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 56603.3516 - val_loss: 36020.9258\n",
      "Epoch 297/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 55367.0391 - val_loss: 34838.5391\n",
      "Epoch 298/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 54163.8359 - val_loss: 33695.0781\n",
      "Epoch 299/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 52992.2422 - val_loss: 32588.6953\n",
      "Epoch 300/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 51851.2812 - val_loss: 31518.1914\n",
      "Epoch 301/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 50739.8828 - val_loss: 30481.9766\n",
      "Epoch 302/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 49657.1562 - val_loss: 29478.7285\n",
      "Epoch 303/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 48602.0820 - val_loss: 28507.3945\n",
      "Epoch 304/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 47574.0586 - val_loss: 27566.8203\n",
      "Epoch 305/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 46572.3789 - val_loss: 26656.1914\n",
      "Epoch 306/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 45596.1094 - val_loss: 25774.3730\n",
      "Epoch 307/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 44644.6406 - val_loss: 24920.7441\n",
      "Epoch 308/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 43717.3516 - val_loss: 24094.1445\n",
      "Epoch 309/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 42813.5781 - val_loss: 23294.3750\n",
      "Epoch 310/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 41932.8750 - val_loss: 22520.3574\n",
      "Epoch 311/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 41074.6406 - val_loss: 21771.6074\n",
      "Epoch 312/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 40238.3867 - val_loss: 21047.4824\n",
      "Epoch 313/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 39423.5312 - val_loss: 20347.4570\n",
      "Epoch 314/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 38629.7070 - val_loss: 19670.8984\n",
      "Epoch 315/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 37856.2969 - val_loss: 19017.2559\n",
      "Epoch 316/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 37103.0078 - val_loss: 18386.4355\n",
      "Epoch 317/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 36369.5859 - val_loss: 17777.5488\n",
      "Epoch 318/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 35655.3984 - val_loss: 17190.3750\n",
      "Epoch 319/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 34960.2266 - val_loss: 16624.5195\n",
      "Epoch 320/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 34283.6602 - val_loss: 16079.5439\n",
      "Epoch 321/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 33625.3750 - val_loss: 15555.1670\n",
      "Epoch 322/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 32984.8477 - val_loss: 15050.9189\n",
      "Epoch 323/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 32362.0898 - val_loss: 14566.6572\n",
      "Epoch 324/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 31756.5254 - val_loss: 14102.2637\n",
      "Epoch 325/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 31168.0547 - val_loss: 13657.2197\n",
      "Epoch 326/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 30596.2129 - val_loss: 13231.5889\n",
      "Epoch 327/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 30040.8203 - val_loss: 12824.9639\n",
      "Epoch 328/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 29501.5176 - val_loss: 12437.3164\n",
      "Epoch 329/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 28978.0273 - val_loss: 12068.1133\n",
      "Epoch 330/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 28469.9590 - val_loss: 11717.5713\n",
      "Epoch 331/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 27977.1543 - val_loss: 11385.2129\n",
      "Epoch 332/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 27499.2129 - val_loss: 11070.6836\n",
      "Epoch 333/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 27035.7441 - val_loss: 10773.6113\n",
      "Epoch 334/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 26586.3516 - val_loss: 10493.5635\n",
      "Epoch 335/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 26150.6250 - val_loss: 10229.8984\n",
      "Epoch 336/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 25727.9980 - val_loss: 9981.9541\n",
      "Epoch 337/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 25317.9492 - val_loss: 9748.8867\n",
      "Epoch 338/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 24920.0859 - val_loss: 9530.0010\n",
      "Epoch 339/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 24533.6641 - val_loss: 9324.0156\n",
      "Epoch 340/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 24158.1875 - val_loss: 9130.1172\n",
      "Epoch 341/700\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 23792.8711 - val_loss: 8946.9746\n",
      "Epoch 342/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 23437.1289 - val_loss: 8773.6191\n",
      "Epoch 343/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 23090.2754 - val_loss: 8608.4697\n",
      "Epoch 344/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 22751.5762 - val_loss: 8450.7412\n",
      "Epoch 345/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 22420.3867 - val_loss: 8299.0615\n",
      "Epoch 346/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 22095.9043 - val_loss: 8152.4292\n",
      "Epoch 347/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21777.6641 - val_loss: 8009.8677\n",
      "Epoch 348/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 21464.9648 - val_loss: 7870.1831\n",
      "Epoch 349/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21157.2930 - val_loss: 7732.9062\n",
      "Epoch 350/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 20854.0039 - val_loss: 7597.0845\n",
      "Epoch 351/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 20554.6621 - val_loss: 7462.1528\n",
      "Epoch 352/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 20258.7344 - val_loss: 7327.5605\n",
      "Epoch 353/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 19966.0156 - val_loss: 7193.1343\n",
      "Epoch 354/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 19676.1289 - val_loss: 7058.5615\n",
      "Epoch 355/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 19389.0840 - val_loss: 6923.8115\n",
      "Epoch 356/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 19104.7617 - val_loss: 6789.0430\n",
      "Epoch 357/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 18823.3164 - val_loss: 6654.5928\n",
      "Epoch 358/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 18545.0723 - val_loss: 6520.8828\n",
      "Epoch 359/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 18270.4375 - val_loss: 6388.6802\n",
      "Epoch 360/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 17999.9004 - val_loss: 6258.6055\n",
      "Epoch 361/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 17734.0156 - val_loss: 6131.1035\n",
      "Epoch 362/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 17473.2383 - val_loss: 6006.9087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 17217.9297 - val_loss: 5886.1602\n",
      "Epoch 364/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16968.3945 - val_loss: 5769.4082\n",
      "Epoch 365/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16724.8789 - val_loss: 5656.6260\n",
      "Epoch 366/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16487.2578 - val_loss: 5547.5181\n",
      "Epoch 367/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16255.4580 - val_loss: 5442.0947\n",
      "Epoch 368/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16029.2588 - val_loss: 5339.9907\n",
      "Epoch 369/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 15808.4326 - val_loss: 5240.9688\n",
      "Epoch 370/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 15592.7295 - val_loss: 5144.6982\n",
      "Epoch 371/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 15381.9463 - val_loss: 5051.0825\n",
      "Epoch 372/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 15175.8320 - val_loss: 4959.5864\n",
      "Epoch 373/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14974.1279 - val_loss: 4870.3496\n",
      "Epoch 374/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14776.7100 - val_loss: 4782.7661\n",
      "Epoch 375/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14583.3750 - val_loss: 4697.2373\n",
      "Epoch 376/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14394.0283 - val_loss: 4613.2236\n",
      "Epoch 377/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14208.4990 - val_loss: 4530.6772\n",
      "Epoch 378/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 14026.5654 - val_loss: 4449.5732\n",
      "Epoch 379/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 13848.2451 - val_loss: 4369.7134\n",
      "Epoch 380/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 13673.3848 - val_loss: 4291.1450\n",
      "Epoch 381/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 13501.8623 - val_loss: 4213.7354\n",
      "Epoch 382/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 13333.5537 - val_loss: 4137.2817\n",
      "Epoch 383/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 13168.3037 - val_loss: 4061.9243\n",
      "Epoch 384/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 13006.0977 - val_loss: 3987.4329\n",
      "Epoch 385/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 12846.8076 - val_loss: 3913.8467\n",
      "Epoch 386/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 12690.3936 - val_loss: 3841.1545\n",
      "Epoch 387/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 12536.7080 - val_loss: 3769.1194\n",
      "Epoch 388/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 12385.6172 - val_loss: 3697.5796\n",
      "Epoch 389/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 12237.0137 - val_loss: 3626.7788\n",
      "Epoch 390/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 12090.8760 - val_loss: 3556.4702\n",
      "Epoch 391/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11947.0947 - val_loss: 3486.5649\n",
      "Epoch 392/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11805.5127 - val_loss: 3417.0620\n",
      "Epoch 393/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11666.0508 - val_loss: 3347.7639\n",
      "Epoch 394/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11528.6260 - val_loss: 3278.6838\n",
      "Epoch 395/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11393.0645 - val_loss: 3209.6567\n",
      "Epoch 396/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11259.3906 - val_loss: 3140.8218\n",
      "Epoch 397/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 11127.3936 - val_loss: 3071.8154\n",
      "Epoch 398/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10997.0322 - val_loss: 3002.6409\n",
      "Epoch 399/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10868.1025 - val_loss: 2933.1587\n",
      "Epoch 400/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 10740.5605 - val_loss: 2863.3828\n",
      "Epoch 401/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10614.2959 - val_loss: 2793.1714\n",
      "Epoch 402/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10489.1992 - val_loss: 2722.3508\n",
      "Epoch 403/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10365.0645 - val_loss: 2650.9031\n",
      "Epoch 404/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 10241.8535 - val_loss: 2578.6895\n",
      "Epoch 405/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 10119.4434 - val_loss: 2505.7463\n",
      "Epoch 406/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9997.7334 - val_loss: 2431.8701\n",
      "Epoch 407/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9876.5557 - val_loss: 2357.0864\n",
      "Epoch 408/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9755.7656 - val_loss: 2281.3904\n",
      "Epoch 409/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9635.3633 - val_loss: 2204.7671\n",
      "Epoch 410/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 9515.1855 - val_loss: 2127.1938\n",
      "Epoch 411/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 9395.0889 - val_loss: 2048.8555\n",
      "Epoch 412/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9275.0752 - val_loss: 1969.8857\n",
      "Epoch 413/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9155.0459 - val_loss: 1890.6968\n",
      "Epoch 414/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9035.0410 - val_loss: 1811.2627\n",
      "Epoch 415/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8915.0078 - val_loss: 1732.3502\n",
      "Epoch 416/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8795.0244 - val_loss: 1654.1840\n",
      "Epoch 417/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8675.1943 - val_loss: 1577.6229\n",
      "Epoch 418/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8555.7305 - val_loss: 1503.2776\n",
      "Epoch 419/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8436.8330 - val_loss: 1431.8834\n",
      "Epoch 420/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8318.7832 - val_loss: 1364.2563\n",
      "Epoch 421/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8201.9229 - val_loss: 1301.0195\n",
      "Epoch 422/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8086.6509 - val_loss: 1243.0767\n",
      "Epoch 423/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7973.3779 - val_loss: 1190.7180\n",
      "Epoch 424/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7862.5332 - val_loss: 1144.3680\n",
      "Epoch 425/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7754.4443 - val_loss: 1104.0486\n",
      "Epoch 426/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7649.4429 - val_loss: 1069.6609\n",
      "Epoch 427/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7547.7148 - val_loss: 1040.7842\n",
      "Epoch 428/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7449.3848 - val_loss: 1016.9003\n",
      "Epoch 429/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7354.4253 - val_loss: 997.3218\n",
      "Epoch 430/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 7262.7310 - val_loss: 981.3491\n",
      "Epoch 431/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7174.1104 - val_loss: 968.3852\n",
      "Epoch 432/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 7088.3237 - val_loss: 957.7365\n",
      "Epoch 433/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 7005.1587 - val_loss: 948.8725\n",
      "Epoch 434/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6924.3228 - val_loss: 941.3997\n",
      "Epoch 435/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6845.5522 - val_loss: 934.9812\n",
      "Epoch 436/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6768.6738 - val_loss: 929.3004\n",
      "Epoch 437/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6693.4219 - val_loss: 924.2495\n",
      "Epoch 438/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 10ms/step - loss: 6619.7017 - val_loss: 919.6213\n",
      "Epoch 439/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6547.3389 - val_loss: 915.3763\n",
      "Epoch 440/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 6476.2295 - val_loss: 911.4636\n",
      "Epoch 441/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6406.3364 - val_loss: 907.8123\n",
      "Epoch 442/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6337.6196 - val_loss: 904.4164\n",
      "Epoch 443/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6269.9858 - val_loss: 901.2860\n",
      "Epoch 444/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6203.5049 - val_loss: 898.4246\n",
      "Epoch 445/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 6138.1089 - val_loss: 895.8203\n",
      "Epoch 446/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6073.8164 - val_loss: 893.4664\n",
      "Epoch 447/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 6010.5776 - val_loss: 891.3596\n",
      "Epoch 448/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5948.3843 - val_loss: 889.5032\n",
      "Epoch 449/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5887.2271 - val_loss: 887.9298\n",
      "Epoch 450/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5827.0176 - val_loss: 886.5102\n",
      "Epoch 451/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5767.7896 - val_loss: 885.3289\n",
      "Epoch 452/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5709.4810 - val_loss: 884.3568\n",
      "Epoch 453/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5652.0151 - val_loss: 883.5388\n",
      "Epoch 454/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5595.4214 - val_loss: 882.8867\n",
      "Epoch 455/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5539.6611 - val_loss: 882.3995\n",
      "Epoch 456/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5484.6821 - val_loss: 882.0171\n",
      "Epoch 457/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5430.4966 - val_loss: 881.7600\n",
      "Epoch 458/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5377.0723 - val_loss: 881.6172\n",
      "Epoch 459/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5324.3794 - val_loss: 881.6166\n",
      "Epoch 460/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5272.4761 - val_loss: 881.7043\n",
      "Epoch 461/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5221.2051 - val_loss: 881.8899\n",
      "Epoch 462/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5170.7007 - val_loss: 882.1911\n",
      "Epoch 463/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5120.8125 - val_loss: 882.6000\n",
      "Epoch 464/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5071.7305 - val_loss: 883.1318\n",
      "Epoch 465/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 5023.2666 - val_loss: 883.8176\n",
      "Epoch 466/700\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 4975.5342 - val_loss: 884.6730\n",
      "Epoch 467/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4928.5103 - val_loss: 885.7084\n",
      "Epoch 468/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4882.1606 - val_loss: 886.9401\n",
      "Epoch 469/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4836.5039 - val_loss: 888.4269\n",
      "Epoch 470/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4791.5654 - val_loss: 890.1865\n",
      "Epoch 471/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4747.3262 - val_loss: 892.2172\n",
      "Epoch 472/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4703.6548 - val_loss: 894.5432\n",
      "Epoch 473/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4660.5635 - val_loss: 897.1878\n",
      "Epoch 474/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4617.9990 - val_loss: 900.1367\n",
      "Epoch 475/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4575.8975 - val_loss: 903.3629\n",
      "Epoch 476/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4534.1851 - val_loss: 906.8774\n",
      "Epoch 477/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4492.7729 - val_loss: 910.6559\n",
      "Epoch 478/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4451.6362 - val_loss: 914.6677\n",
      "Epoch 479/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4410.8584 - val_loss: 918.8468\n",
      "Epoch 480/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 4370.1924 - val_loss: 923.2282\n",
      "Epoch 481/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4329.6460 - val_loss: 927.7261\n",
      "Epoch 482/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4289.3013 - val_loss: 932.3574\n",
      "Epoch 483/700\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 4249.1245 - val_loss: 937.1309\n",
      "Epoch 484/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4209.0303 - val_loss: 941.9423\n",
      "Epoch 485/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4169.0220 - val_loss: 946.8192\n",
      "Epoch 486/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4129.1914 - val_loss: 951.7846\n",
      "Epoch 487/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4089.5186 - val_loss: 956.7689\n",
      "Epoch 488/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 4049.9773 - val_loss: 961.7781\n",
      "Epoch 489/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 4010.6133 - val_loss: 966.8314\n",
      "Epoch 490/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3971.3762 - val_loss: 971.8522\n",
      "Epoch 491/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3932.3650 - val_loss: 976.9320\n",
      "Epoch 492/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3893.5525 - val_loss: 982.0043\n",
      "Epoch 493/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3854.9722 - val_loss: 987.0238\n",
      "Epoch 494/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3816.6357 - val_loss: 992.0032\n",
      "Epoch 495/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3778.4165 - val_loss: 997.0280\n",
      "Epoch 496/700\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 3740.5247 - val_loss: 1002.0739\n",
      "Epoch 497/700\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3702.9614 - val_loss: 1007.0485\n",
      "Epoch 498/700\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 4037.5332"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40, min_delta=0.001)\n",
    "history_tech=model_tech.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=700,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=[es]\n",
    ")\n",
    "y_pred = model_tech.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_tech.history['loss'], label='train')\n",
    "plt.plot(history_tech.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(y_train)), y_train, 'g', label=\"history\")\n",
    "plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test, marker='.', label=\"true\")\n",
    "plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred, 'r', label=\"prediction\")\n",
    "# plt.plot(np.arange(0, len(y_train)), sc_y.inverse_transform(y_train), 'g', label=\"history\")\n",
    "# plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), sc_y.inverse_transform(y_test), marker='.', label=\"true\")\n",
    "# plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), sc_y.inverse_transform(y_pred), 'r', label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134553b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_test = sc.inverse_transform(y_test)\n",
    "# y_pred = sc.inverse_transform(y_pred)\n",
    "plt.plot(y_test, marker='.', label=\"true\")\n",
    "plt.plot(y_pred, 'r', label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf212a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,y_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=data_val.drop('Prediction',1).values\n",
    "y_val=data_val['Prediction']\n",
    "# x_val=x_val.reshape(x_val.shape[0],1,x_val.shape[1])\n",
    "x_val=sc.transform(x_val).reshape(x_val.shape[0],1,x_val.shape[1])\n",
    "y_val_pred=model_tech.predict(x_val)\n",
    "# y_val_pred=sc_y.inverse_transform(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_val.values, marker='.', label=\"true\")\n",
    "plt.plot(y_val_pred, 'r', marker='*',label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_val.values,y_val_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab65e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
