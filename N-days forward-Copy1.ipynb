{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcbd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler ,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score, mean_squared_error,mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import LSTM,Dropout, BatchNormalization, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.optimizer_v2.rmsprop import RMSprop\n",
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4562be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-09-16</th>\n",
       "      <td>40.599998</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>39.599998</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>19553563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17</th>\n",
       "      <td>39.700001</td>\n",
       "      <td>40.400002</td>\n",
       "      <td>39.099998</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>28266370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-20</th>\n",
       "      <td>39.900002</td>\n",
       "      <td>39.950001</td>\n",
       "      <td>38.950001</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>23796781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21</th>\n",
       "      <td>38.349998</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.299999</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>18117762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-23</th>\n",
       "      <td>39.299999</td>\n",
       "      <td>39.950001</td>\n",
       "      <td>39.200001</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>20518033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume\n",
       "Date                                                                       \n",
       "2021-09-16  40.599998  40.599998  39.599998  40.000000  40.000000  19553563\n",
       "2021-09-17  39.700001  40.400002  39.099998  40.250000  40.250000  28266370\n",
       "2021-09-20  39.900002  39.950001  38.950001  39.000000  39.000000  23796781\n",
       "2021-09-21  38.349998  39.000000  38.299999  38.799999  38.799999  18117762\n",
       "2021-09-23  39.299999  39.950001  39.200001  39.750000  39.750000  20518033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for the SPY ETF by specifying the stock ticker, start date, and end date\n",
    "data = yf.download('0005.hk',\"2017-09-28\",\"2021-09-24\")\n",
    "# data.drop(\"Adj Close\",axis=1,inplace=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff9d407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>76.400002</td>\n",
       "      <td>77.099998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.199997</td>\n",
       "      <td>64.386986</td>\n",
       "      <td>22520013</td>\n",
       "      <td>76.550003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>76.500000</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.300003</td>\n",
       "      <td>76.550003</td>\n",
       "      <td>64.682732</td>\n",
       "      <td>16766252</td>\n",
       "      <td>77.449997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-03</th>\n",
       "      <td>76.800003</td>\n",
       "      <td>77.449997</td>\n",
       "      <td>76.699997</td>\n",
       "      <td>77.449997</td>\n",
       "      <td>65.443207</td>\n",
       "      <td>24369212</td>\n",
       "      <td>77.849998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04</th>\n",
       "      <td>77.500000</td>\n",
       "      <td>77.949997</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>77.849998</td>\n",
       "      <td>65.781197</td>\n",
       "      <td>21197563</td>\n",
       "      <td>77.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>77.599998</td>\n",
       "      <td>77.800003</td>\n",
       "      <td>77.199997</td>\n",
       "      <td>77.349998</td>\n",
       "      <td>65.358704</td>\n",
       "      <td>23715110</td>\n",
       "      <td>77.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-15</th>\n",
       "      <td>40.599998</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.099998</td>\n",
       "      <td>40.099998</td>\n",
       "      <td>14418368</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-16</th>\n",
       "      <td>40.599998</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>39.599998</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>19553563</td>\n",
       "      <td>40.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17</th>\n",
       "      <td>39.700001</td>\n",
       "      <td>40.400002</td>\n",
       "      <td>39.099998</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>28266370</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-20</th>\n",
       "      <td>39.900002</td>\n",
       "      <td>39.950001</td>\n",
       "      <td>38.950001</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>23796781</td>\n",
       "      <td>38.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21</th>\n",
       "      <td>38.349998</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.299999</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>18117762</td>\n",
       "      <td>39.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2017-09-28  76.400002  77.099998  76.050003  76.199997  64.386986  22520013   \n",
       "2017-09-29  76.500000  76.900002  76.300003  76.550003  64.682732  16766252   \n",
       "2017-10-03  76.800003  77.449997  76.699997  77.449997  65.443207  24369212   \n",
       "2017-10-04  77.500000  77.949997  77.500000  77.849998  65.781197  21197563   \n",
       "2017-10-06  77.599998  77.800003  77.199997  77.349998  65.358704  23715110   \n",
       "...               ...        ...        ...        ...        ...       ...   \n",
       "2021-09-15  40.599998  40.599998  40.000000  40.099998  40.099998  14418368   \n",
       "2021-09-16  40.599998  40.599998  39.599998  40.000000  40.000000  19553563   \n",
       "2021-09-17  39.700001  40.400002  39.099998  40.250000  40.250000  28266370   \n",
       "2021-09-20  39.900002  39.950001  38.950001  39.000000  39.000000  23796781   \n",
       "2021-09-21  38.349998  39.000000  38.299999  38.799999  38.799999  18117762   \n",
       "\n",
       "            prediction  \n",
       "Date                    \n",
       "2017-09-28   76.550003  \n",
       "2017-09-29   77.449997  \n",
       "2017-10-03   77.849998  \n",
       "2017-10-04   77.349998  \n",
       "2017-10-06   77.500000  \n",
       "...                ...  \n",
       "2021-09-15   40.000000  \n",
       "2021-09-16   40.250000  \n",
       "2021-09-17   39.000000  \n",
       "2021-09-20   38.799999  \n",
       "2021-09-21   39.750000  \n",
       "\n",
       "[982 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prediction'] = data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75312b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.64000015e+01 7.70999985e+01 7.60500031e+01 7.61999969e+01\n",
      "  6.43869858e+01 2.25200130e+07 7.65500031e+01]\n",
      " [7.65000000e+01 7.69000015e+01 7.63000031e+01 7.65500031e+01\n",
      "  6.46827316e+01 1.67662520e+07 7.74499969e+01]\n",
      " [7.68000031e+01 7.74499969e+01 7.66999969e+01 7.74499969e+01\n",
      "  6.54432068e+01 2.43692120e+07 7.78499985e+01]\n",
      " [7.75000000e+01 7.79499969e+01 7.75000000e+01 7.78499985e+01\n",
      "  6.57811966e+01 2.11975630e+07 7.73499985e+01]\n",
      " [7.75999985e+01 7.78000031e+01 7.71999969e+01 7.73499985e+01\n",
      "  6.53587036e+01 2.37151100e+07 7.75000000e+01]\n",
      " [7.73499985e+01 7.76999969e+01 7.70999985e+01 7.75000000e+01\n",
      "  6.54854507e+01 1.65523540e+07 7.85999985e+01]]\n"
     ]
    }
   ],
   "source": [
    "data_price=data[\"prediction\"]\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "def processData(data,data_price,lb,forecast_day):\n",
    "    X,Y = [],[]\n",
    "    for i in range(lb-1,len(data)-lb-1):\n",
    "        X.append(data[i-(lb-1):i])\n",
    "        Y.append(data_price[i:(i+forecast_day)])\n",
    "    return np.array(X),np.array(Y)\n",
    "\n",
    "lb=7\n",
    "forecast=5\n",
    "X,y = processData(data.dropna().values,data_price,lb,forecast)\n",
    "X_train,X_test = X[:int(X.shape[0]*0.90)],X[int(X.shape[0]*0.90):]\n",
    "y_train,y_test = y[:int(y.shape[0]*0.90)],y[int(y.shape[0]*0.90):]\n",
    "# print(type(X_train))\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78bead1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>76.400002</td>\n",
       "      <td>77.099998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.199997</td>\n",
       "      <td>64.386986</td>\n",
       "      <td>22520013</td>\n",
       "      <td>76.550003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>76.500000</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.300003</td>\n",
       "      <td>76.550003</td>\n",
       "      <td>64.682732</td>\n",
       "      <td>16766252</td>\n",
       "      <td>77.449997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-03</th>\n",
       "      <td>76.800003</td>\n",
       "      <td>77.449997</td>\n",
       "      <td>76.699997</td>\n",
       "      <td>77.449997</td>\n",
       "      <td>65.443207</td>\n",
       "      <td>24369212</td>\n",
       "      <td>77.849998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04</th>\n",
       "      <td>77.500000</td>\n",
       "      <td>77.949997</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>77.849998</td>\n",
       "      <td>65.781197</td>\n",
       "      <td>21197563</td>\n",
       "      <td>77.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>77.599998</td>\n",
       "      <td>77.800003</td>\n",
       "      <td>77.199997</td>\n",
       "      <td>77.349998</td>\n",
       "      <td>65.358704</td>\n",
       "      <td>23715110</td>\n",
       "      <td>77.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-09</th>\n",
       "      <td>77.349998</td>\n",
       "      <td>77.699997</td>\n",
       "      <td>77.099998</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>65.485451</td>\n",
       "      <td>16552354</td>\n",
       "      <td>78.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10</th>\n",
       "      <td>77.500000</td>\n",
       "      <td>79.650002</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>78.599998</td>\n",
       "      <td>66.414932</td>\n",
       "      <td>54726340</td>\n",
       "      <td>78.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-11</th>\n",
       "      <td>79.050003</td>\n",
       "      <td>79.199997</td>\n",
       "      <td>78.099998</td>\n",
       "      <td>78.099998</td>\n",
       "      <td>65.992439</td>\n",
       "      <td>33386628</td>\n",
       "      <td>77.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-12</th>\n",
       "      <td>77.699997</td>\n",
       "      <td>77.750000</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>65.975250</td>\n",
       "      <td>14127044</td>\n",
       "      <td>77.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-13</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.199997</td>\n",
       "      <td>76.650002</td>\n",
       "      <td>77.199997</td>\n",
       "      <td>65.889885</td>\n",
       "      <td>16107262</td>\n",
       "      <td>77.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-16</th>\n",
       "      <td>77.199997</td>\n",
       "      <td>77.750000</td>\n",
       "      <td>77.050003</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>65.975250</td>\n",
       "      <td>15033599</td>\n",
       "      <td>77.150002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-17</th>\n",
       "      <td>77.099998</td>\n",
       "      <td>77.449997</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.150002</td>\n",
       "      <td>65.847214</td>\n",
       "      <td>10181513</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-18</th>\n",
       "      <td>76.849998</td>\n",
       "      <td>77.099998</td>\n",
       "      <td>76.750000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>65.719200</td>\n",
       "      <td>11810048</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-19</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>64.865700</td>\n",
       "      <td>22311059</td>\n",
       "      <td>76.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-20</th>\n",
       "      <td>76.500000</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.400002</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>65.292435</td>\n",
       "      <td>13357000</td>\n",
       "      <td>76.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2017-09-28  76.400002  77.099998  76.050003  76.199997  64.386986  22520013   \n",
       "2017-09-29  76.500000  76.900002  76.300003  76.550003  64.682732  16766252   \n",
       "2017-10-03  76.800003  77.449997  76.699997  77.449997  65.443207  24369212   \n",
       "2017-10-04  77.500000  77.949997  77.500000  77.849998  65.781197  21197563   \n",
       "2017-10-06  77.599998  77.800003  77.199997  77.349998  65.358704  23715110   \n",
       "2017-10-09  77.349998  77.699997  77.099998  77.500000  65.485451  16552354   \n",
       "2017-10-10  77.500000  79.650002  77.500000  78.599998  66.414932  54726340   \n",
       "2017-10-11  79.050003  79.199997  78.099998  78.099998  65.992439  33386628   \n",
       "2017-10-12  77.699997  77.750000  77.300003  77.300003  65.975250  14127044   \n",
       "2017-10-13  77.000000  77.199997  76.650002  77.199997  65.889885  16107262   \n",
       "2017-10-16  77.199997  77.750000  77.050003  77.300003  65.975250  15033599   \n",
       "2017-10-17  77.099998  77.449997  77.000000  77.150002  65.847214  10181513   \n",
       "2017-10-18  76.849998  77.099998  76.750000  77.000000  65.719200  11810048   \n",
       "2017-10-19  77.000000  77.250000  76.000000  76.000000  64.865700  22311059   \n",
       "2017-10-20  76.500000  76.900002  76.400002  76.500000  65.292435  13357000   \n",
       "\n",
       "            prediction  \n",
       "Date                    \n",
       "2017-09-28   76.550003  \n",
       "2017-09-29   77.449997  \n",
       "2017-10-03   77.849998  \n",
       "2017-10-04   77.349998  \n",
       "2017-10-06   77.500000  \n",
       "2017-10-09   78.599998  \n",
       "2017-10-10   78.099998  \n",
       "2017-10-11   77.300003  \n",
       "2017-10-12   77.199997  \n",
       "2017-10-13   77.300003  \n",
       "2017-10-16   77.150002  \n",
       "2017-10-17   77.000000  \n",
       "2017-10-18   76.000000  \n",
       "2017-10-19   76.500000  \n",
       "2017-10-20   76.500000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f0b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape= (X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(\n",
    "  loss=\"mean_squared_error\",\n",
    "  optimizer='Adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e20b754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "44/44 [==============================] - 10s 20ms/step - loss: 4409.2638 - val_loss: 904.9272\n",
      "Epoch 2/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 2957.9253 - val_loss: 615.1413\n",
      "Epoch 3/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 2462.7084 - val_loss: 427.3309\n",
      "Epoch 4/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 2088.8226 - val_loss: 287.9947\n",
      "Epoch 5/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 1779.9126 - val_loss: 185.2335\n",
      "Epoch 6/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 1511.1291 - val_loss: 111.7632\n",
      "Epoch 7/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 1308.3190 - val_loss: 61.8778\n",
      "Epoch 8/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 1118.2225 - val_loss: 30.6932\n",
      "Epoch 9/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 955.7366 - val_loss: 14.0353\n",
      "Epoch 10/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 830.0766 - val_loss: 8.4115\n",
      "Epoch 11/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 737.4382 - val_loss: 10.8020\n",
      "Epoch 12/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 645.3705 - val_loss: 18.8138\n",
      "Epoch 13/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 576.8438 - val_loss: 30.3548\n",
      "Epoch 14/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 521.0359 - val_loss: 44.0880\n",
      "Epoch 15/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 467.7959 - val_loss: 58.7820\n",
      "Epoch 16/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 436.6097 - val_loss: 73.7883\n",
      "Epoch 17/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 396.1830 - val_loss: 87.9087\n",
      "Epoch 18/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 369.9231 - val_loss: 101.0677\n",
      "Epoch 19/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 360.9026 - val_loss: 112.8628\n",
      "Epoch 20/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 335.8316 - val_loss: 123.1347\n",
      "Epoch 21/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 323.8997 - val_loss: 131.8172\n",
      "Epoch 22/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 318.9999 - val_loss: 139.8743\n",
      "Epoch 23/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 319.0565 - val_loss: 146.9050\n",
      "Epoch 24/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 294.2009 - val_loss: 151.8993\n",
      "Epoch 25/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 283.3647 - val_loss: 157.3611\n",
      "Epoch 26/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 283.3813 - val_loss: 160.2843\n",
      "Epoch 27/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 296.1317 - val_loss: 163.1286\n",
      "Epoch 28/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 288.4469 - val_loss: 144.9783\n",
      "Epoch 29/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 285.0423 - val_loss: 164.0992\n",
      "Epoch 30/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 280.3508 - val_loss: 167.2251\n",
      "Epoch 31/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.3942 - val_loss: 166.7918\n",
      "Epoch 32/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 278.1630 - val_loss: 173.6136\n",
      "Epoch 33/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 278.2117 - val_loss: 175.7738\n",
      "Epoch 34/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.1669 - val_loss: 176.5872\n",
      "Epoch 35/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.4486 - val_loss: 177.7697\n",
      "Epoch 36/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 278.8977 - val_loss: 179.0159\n",
      "Epoch 37/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.4328 - val_loss: 179.2755\n",
      "Epoch 38/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 257.3979 - val_loss: 179.6688\n",
      "Epoch 39/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.8518 - val_loss: 179.4255\n",
      "Epoch 40/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.6283 - val_loss: 180.8046\n",
      "Epoch 41/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.9048 - val_loss: 180.0036\n",
      "Epoch 42/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.0422 - val_loss: 180.9543\n",
      "Epoch 43/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.3825 - val_loss: 181.5426\n",
      "Epoch 44/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.9391 - val_loss: 182.8717\n",
      "Epoch 45/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 272.7290 - val_loss: 183.8065\n",
      "Epoch 46/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.4930 - val_loss: 183.2415\n",
      "Epoch 47/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.0231 - val_loss: 182.5507\n",
      "Epoch 48/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.4278 - val_loss: 182.8482\n",
      "Epoch 49/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.0755 - val_loss: 183.6318\n",
      "Epoch 50/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.3029 - val_loss: 183.4777\n",
      "Epoch 51/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 257.4466 - val_loss: 183.0029\n",
      "Epoch 52/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.5119 - val_loss: 182.8908\n",
      "Epoch 53/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.0762 - val_loss: 182.6569\n",
      "Epoch 54/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.9346 - val_loss: 183.0271\n",
      "Epoch 55/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.8132 - val_loss: 182.6422\n",
      "Epoch 56/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.9726 - val_loss: 182.0430\n",
      "Epoch 57/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.4368 - val_loss: 182.3497\n",
      "Epoch 58/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.1977 - val_loss: 182.1120\n",
      "Epoch 59/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.3215 - val_loss: 181.5053\n",
      "Epoch 60/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.1830 - val_loss: 181.2924\n",
      "Epoch 61/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.4201 - val_loss: 181.3010\n",
      "Epoch 62/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.7921 - val_loss: 181.8180\n",
      "Epoch 63/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.8515 - val_loss: 182.4955\n",
      "Epoch 64/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.1859 - val_loss: 183.4017\n",
      "Epoch 65/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 272.3628 - val_loss: 183.3853\n",
      "Epoch 66/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.4699 - val_loss: 183.0527\n",
      "Epoch 67/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.2234 - val_loss: 183.2465\n",
      "Epoch 68/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 277.1336 - val_loss: 183.4921\n",
      "Epoch 69/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 257.7552 - val_loss: 182.1690\n",
      "Epoch 70/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.3682 - val_loss: 181.4097\n",
      "Epoch 71/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.0272 - val_loss: 181.3864\n",
      "Epoch 72/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.9658 - val_loss: 182.2838\n",
      "Epoch 73/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.3239 - val_loss: 182.1167\n",
      "Epoch 74/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.0351 - val_loss: 182.7066\n",
      "Epoch 75/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.3039 - val_loss: 182.8026\n",
      "Epoch 76/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.9428 - val_loss: 183.0701\n",
      "Epoch 77/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 280.7993 - val_loss: 183.6941\n",
      "Epoch 78/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.5222 - val_loss: 183.2936\n",
      "Epoch 79/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 259.2171 - val_loss: 182.5028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.4981 - val_loss: 181.7872\n",
      "Epoch 81/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.0310 - val_loss: 180.9335\n",
      "Epoch 82/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 273.0454 - val_loss: 181.7556\n",
      "Epoch 83/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 265.8360 - val_loss: 181.2229\n",
      "Epoch 84/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.3760 - val_loss: 181.1848\n",
      "Epoch 85/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.8326 - val_loss: 182.0091\n",
      "Epoch 86/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.1753 - val_loss: 182.2416\n",
      "Epoch 87/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.1138 - val_loss: 182.3170\n",
      "Epoch 88/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 254.9367 - val_loss: 181.4869\n",
      "Epoch 89/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.0127 - val_loss: 181.3857\n",
      "Epoch 90/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.9705 - val_loss: 181.3201\n",
      "Epoch 91/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.0112 - val_loss: 180.7442\n",
      "Epoch 92/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.1968 - val_loss: 181.1426\n",
      "Epoch 93/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.8522 - val_loss: 182.0169\n",
      "Epoch 94/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 259.8016 - val_loss: 181.4550\n",
      "Epoch 95/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 269.2525 - val_loss: 182.1946\n",
      "Epoch 96/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.3959 - val_loss: 182.4843\n",
      "Epoch 97/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.4237 - val_loss: 183.2190\n",
      "Epoch 98/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6744 - val_loss: 182.9759\n",
      "Epoch 99/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.2749 - val_loss: 182.1187\n",
      "Epoch 100/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 272.0737 - val_loss: 182.1249\n",
      "Epoch 101/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.5650 - val_loss: 183.4398\n",
      "Epoch 102/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 261.1775 - val_loss: 182.9491\n",
      "Epoch 103/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.9372 - val_loss: 181.9700\n",
      "Epoch 104/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.6264 - val_loss: 182.3436\n",
      "Epoch 105/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.2171 - val_loss: 181.7971\n",
      "Epoch 106/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.6190 - val_loss: 182.4349\n",
      "Epoch 107/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.2599 - val_loss: 181.5627\n",
      "Epoch 108/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 259.8582 - val_loss: 180.2468\n",
      "Epoch 109/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.0769 - val_loss: 180.2295\n",
      "Epoch 110/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.5898 - val_loss: 180.0373\n",
      "Epoch 111/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 275.0193 - val_loss: 180.3109\n",
      "Epoch 112/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.1110 - val_loss: 179.7036\n",
      "Epoch 113/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 272.2355 - val_loss: 179.6516\n",
      "Epoch 114/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 275.0684 - val_loss: 180.4965\n",
      "Epoch 115/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.2298 - val_loss: 179.9925\n",
      "Epoch 116/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 262.6107 - val_loss: 180.5513\n",
      "Epoch 117/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.5263 - val_loss: 180.7324\n",
      "Epoch 118/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.7290 - val_loss: 181.4783\n",
      "Epoch 119/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.7005 - val_loss: 181.2665\n",
      "Epoch 120/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.3468 - val_loss: 181.8892\n",
      "Epoch 121/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.7315 - val_loss: 181.5273\n",
      "Epoch 122/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.2030 - val_loss: 182.2286\n",
      "Epoch 123/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.0859 - val_loss: 182.8048\n",
      "Epoch 124/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.4530 - val_loss: 182.8472\n",
      "Epoch 125/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.3096 - val_loss: 183.8281\n",
      "Epoch 126/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.3435 - val_loss: 182.5163\n",
      "Epoch 127/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.2343 - val_loss: 181.3131\n",
      "Epoch 128/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6648 - val_loss: 181.5021\n",
      "Epoch 129/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.6427 - val_loss: 180.6286\n",
      "Epoch 130/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.4525 - val_loss: 180.8978\n",
      "Epoch 131/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 270.4855 - val_loss: 181.1066\n",
      "Epoch 132/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 276.2314 - val_loss: 181.9041\n",
      "Epoch 133/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.8391 - val_loss: 182.2370\n",
      "Epoch 134/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.6596 - val_loss: 182.6990\n",
      "Epoch 135/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.0165 - val_loss: 182.2297\n",
      "Epoch 136/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.8670 - val_loss: 182.0499\n",
      "Epoch 137/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.4515 - val_loss: 181.4707\n",
      "Epoch 138/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.3103 - val_loss: 180.4629\n",
      "Epoch 139/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 272.9701 - val_loss: 181.0407\n",
      "Epoch 140/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.7301 - val_loss: 180.2210\n",
      "Epoch 141/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.8551 - val_loss: 180.1927\n",
      "Epoch 142/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.0377 - val_loss: 180.1030\n",
      "Epoch 143/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6261 - val_loss: 179.2591\n",
      "Epoch 144/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6785 - val_loss: 180.9852\n",
      "Epoch 145/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.8569 - val_loss: 182.3984\n",
      "Epoch 146/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.6894 - val_loss: 182.4075\n",
      "Epoch 147/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.9197 - val_loss: 183.1471\n",
      "Epoch 148/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.4839 - val_loss: 182.9547\n",
      "Epoch 149/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.4853 - val_loss: 182.6726\n",
      "Epoch 150/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.5654 - val_loss: 182.4018\n",
      "Epoch 151/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.8973 - val_loss: 182.8570\n",
      "Epoch 152/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.0219 - val_loss: 182.0054\n",
      "Epoch 153/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.7754 - val_loss: 182.0408\n",
      "Epoch 154/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.6520 - val_loss: 181.1224\n",
      "Epoch 155/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.4228 - val_loss: 181.1608\n",
      "Epoch 156/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.2420 - val_loss: 180.7395\n",
      "Epoch 157/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.8726 - val_loss: 181.1641\n",
      "Epoch 158/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 6ms/step - loss: 268.3546 - val_loss: 181.5088\n",
      "Epoch 159/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.1853 - val_loss: 181.5855\n",
      "Epoch 160/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 259.0561 - val_loss: 180.5099\n",
      "Epoch 161/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.1607 - val_loss: 180.9589\n",
      "Epoch 162/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.2942 - val_loss: 181.3082\n",
      "Epoch 163/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.8217 - val_loss: 181.5863\n",
      "Epoch 164/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.9620 - val_loss: 181.8275\n",
      "Epoch 165/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.0677 - val_loss: 181.4296\n",
      "Epoch 166/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.8707 - val_loss: 181.1902\n",
      "Epoch 167/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.5640 - val_loss: 180.5812\n",
      "Epoch 168/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.6214 - val_loss: 180.5752\n",
      "Epoch 169/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.9617 - val_loss: 181.1510\n",
      "Epoch 170/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 257.5528 - val_loss: 181.5923\n",
      "Epoch 171/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6706 - val_loss: 181.1917\n",
      "Epoch 172/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.8042 - val_loss: 181.8842\n",
      "Epoch 173/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.2692 - val_loss: 181.9711\n",
      "Epoch 174/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.4072 - val_loss: 182.1733\n",
      "Epoch 175/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.0440 - val_loss: 182.1165\n",
      "Epoch 176/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.4961 - val_loss: 180.9010\n",
      "Epoch 177/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 279.0659 - val_loss: 180.9786\n",
      "Epoch 178/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 272.4485 - val_loss: 181.1201\n",
      "Epoch 179/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.2668 - val_loss: 181.6534\n",
      "Epoch 180/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.2635 - val_loss: 182.4017\n",
      "Epoch 181/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.9099 - val_loss: 182.5467\n",
      "Epoch 182/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.0851 - val_loss: 182.1680\n",
      "Epoch 183/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.7054 - val_loss: 182.3510\n",
      "Epoch 184/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.1961 - val_loss: 182.5687\n",
      "Epoch 185/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.6719 - val_loss: 182.9210\n",
      "Epoch 186/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.5884 - val_loss: 182.6347\n",
      "Epoch 187/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.5028 - val_loss: 182.6061\n",
      "Epoch 188/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.7550 - val_loss: 181.7453\n",
      "Epoch 189/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.1532 - val_loss: 181.5295\n",
      "Epoch 190/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 275.1460 - val_loss: 182.6145\n",
      "Epoch 191/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.5610 - val_loss: 181.6658\n",
      "Epoch 192/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.7556 - val_loss: 180.9398\n",
      "Epoch 193/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 275.6310 - val_loss: 181.6784\n",
      "Epoch 194/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.9534 - val_loss: 180.4448\n",
      "Epoch 195/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.9427 - val_loss: 180.6440\n",
      "Epoch 196/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.3000 - val_loss: 180.8987\n",
      "Epoch 197/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.1511 - val_loss: 180.9916\n",
      "Epoch 198/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 277.2061 - val_loss: 181.7611\n",
      "Epoch 199/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.0064 - val_loss: 182.5173\n",
      "Epoch 200/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.1180 - val_loss: 182.0439\n",
      "Epoch 201/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.2337 - val_loss: 182.0610\n",
      "Epoch 202/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.9349 - val_loss: 114.2541\n",
      "Epoch 203/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 332.8416 - val_loss: 159.6640\n",
      "Epoch 204/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 278.8414 - val_loss: 165.2366\n",
      "Epoch 205/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 287.1557 - val_loss: 168.3475\n",
      "Epoch 206/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.3222 - val_loss: 171.8044\n",
      "Epoch 207/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 278.5382 - val_loss: 174.7751\n",
      "Epoch 208/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.9030 - val_loss: 176.7432\n",
      "Epoch 209/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.8347 - val_loss: 177.6926\n",
      "Epoch 210/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.4485 - val_loss: 178.2609\n",
      "Epoch 211/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.7606 - val_loss: 179.0122\n",
      "Epoch 212/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.3111 - val_loss: 180.2899\n",
      "Epoch 213/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.3882 - val_loss: 180.5685\n",
      "Epoch 214/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.8384 - val_loss: 180.6972\n",
      "Epoch 215/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.5988 - val_loss: 180.8806\n",
      "Epoch 216/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.2348 - val_loss: 180.8492\n",
      "Epoch 217/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.7817 - val_loss: 181.4384\n",
      "Epoch 218/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.1434 - val_loss: 181.9811\n",
      "Epoch 219/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.2616 - val_loss: 181.9717\n",
      "Epoch 220/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.1840 - val_loss: 182.2187\n",
      "Epoch 221/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.9614 - val_loss: 181.2885\n",
      "Epoch 222/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.3221 - val_loss: 181.4480\n",
      "Epoch 223/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.7686 - val_loss: 182.0857\n",
      "Epoch 224/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.7037 - val_loss: 182.5779\n",
      "Epoch 225/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.4928 - val_loss: 182.3933\n",
      "Epoch 226/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.1546 - val_loss: 182.5531\n",
      "Epoch 227/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.1470 - val_loss: 181.8236\n",
      "Epoch 228/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.7756 - val_loss: 181.3547\n",
      "Epoch 229/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.9908 - val_loss: 182.3349\n",
      "Epoch 230/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.1256 - val_loss: 182.6002\n",
      "Epoch 231/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6927 - val_loss: 182.5695\n",
      "Epoch 232/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 254.9656 - val_loss: 182.0505\n",
      "Epoch 233/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.5604 - val_loss: 182.3032\n",
      "Epoch 234/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 254.6066 - val_loss: 182.1578\n",
      "Epoch 235/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.9554 - val_loss: 182.1960\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 6ms/step - loss: 264.9503 - val_loss: 182.4796\n",
      "Epoch 237/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.7821 - val_loss: 182.1240\n",
      "Epoch 238/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.1360 - val_loss: 182.0965\n",
      "Epoch 239/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.9004 - val_loss: 180.9645\n",
      "Epoch 240/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.4215 - val_loss: 180.6159\n",
      "Epoch 241/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 275.1278 - val_loss: 180.6530\n",
      "Epoch 242/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.9608 - val_loss: 179.8508\n",
      "Epoch 243/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.7603 - val_loss: 179.9034\n",
      "Epoch 244/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6816 - val_loss: 179.0644\n",
      "Epoch 245/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.6259 - val_loss: 179.2961\n",
      "Epoch 246/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.5272 - val_loss: 180.0704\n",
      "Epoch 247/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 262.5710 - val_loss: 180.7313\n",
      "Epoch 248/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.0694 - val_loss: 181.4135\n",
      "Epoch 249/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.4950 - val_loss: 181.4361\n",
      "Epoch 250/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.0165 - val_loss: 181.7413\n",
      "Epoch 251/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.7817 - val_loss: 181.3779\n",
      "Epoch 252/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 259.3440 - val_loss: 180.6905\n",
      "Epoch 253/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.9038 - val_loss: 180.9916\n",
      "Epoch 254/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.6241 - val_loss: 181.5040\n",
      "Epoch 255/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.5978 - val_loss: 182.2070\n",
      "Epoch 256/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.6691 - val_loss: 181.9806\n",
      "Epoch 257/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.7689 - val_loss: 181.6033\n",
      "Epoch 258/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.4411 - val_loss: 181.5501\n",
      "Epoch 259/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.8097 - val_loss: 180.7547\n",
      "Epoch 260/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.5629 - val_loss: 180.6739\n",
      "Epoch 261/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.9637 - val_loss: 181.3964\n",
      "Epoch 262/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.4554 - val_loss: 182.1132\n",
      "Epoch 263/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.9847 - val_loss: 182.3596\n",
      "Epoch 264/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.7524 - val_loss: 182.0273\n",
      "Epoch 265/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.6927 - val_loss: 181.8788\n",
      "Epoch 266/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.9440 - val_loss: 181.4034\n",
      "Epoch 267/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 258.7582 - val_loss: 180.6873\n",
      "Epoch 268/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.4085 - val_loss: 180.9732\n",
      "Epoch 269/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 265.6999 - val_loss: 181.1973\n",
      "Epoch 270/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 264.8512 - val_loss: 181.3125\n",
      "Epoch 271/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 275.8210 - val_loss: 181.3657\n",
      "Epoch 272/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.3195 - val_loss: 180.6431\n",
      "Epoch 273/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.9237 - val_loss: 180.5689\n",
      "Epoch 274/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 257.7715 - val_loss: 180.5380\n",
      "Epoch 275/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 259.2353 - val_loss: 180.8301\n",
      "Epoch 276/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.6439 - val_loss: 180.4482\n",
      "Epoch 277/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.6794 - val_loss: 180.3684\n",
      "Epoch 278/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.9781 - val_loss: 180.6686\n",
      "Epoch 279/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 271.9494 - val_loss: 180.6957\n",
      "Epoch 280/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.8841 - val_loss: 181.2257\n",
      "Epoch 281/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.1985 - val_loss: 181.5967\n",
      "Epoch 282/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 276.7037 - val_loss: 181.6210\n",
      "Epoch 283/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.6645 - val_loss: 182.0710\n",
      "Epoch 284/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.2529 - val_loss: 182.0497\n",
      "Epoch 285/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.2351 - val_loss: 181.9498\n",
      "Epoch 286/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 276.5966 - val_loss: 182.7167\n",
      "Epoch 287/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.7706 - val_loss: 182.6825\n",
      "Epoch 288/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 263.0367 - val_loss: 181.7027\n",
      "Epoch 289/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 274.3360 - val_loss: 181.2227\n",
      "Epoch 290/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 265.7940 - val_loss: 181.2057\n",
      "Epoch 291/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 268.8276 - val_loss: 181.0542\n",
      "Epoch 292/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 267.5036 - val_loss: 180.5645\n",
      "Epoch 293/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 266.2467 - val_loss: 180.5723\n",
      "Epoch 294/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 273.0290 - val_loss: 180.3772\n",
      "Epoch 295/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 261.1463 - val_loss: 180.0958\n",
      "Epoch 296/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 266.5118 - val_loss: 180.1550\n",
      "Epoch 297/300\n",
      "44/44 [==============================] - 0s 5ms/step - loss: 261.9151 - val_loss: 178.8777\n",
      "Epoch 298/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 269.6590 - val_loss: 178.8881\n",
      "Epoch 299/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 270.4273 - val_loss: 178.7058\n",
      "Epoch 300/300\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 260.5102 - val_loss: 178.2212\n"
     ]
    }
   ],
   "source": [
    "#Fit model with history to check for overfitting\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40, min_delta=0.0001)\n",
    "history = model.fit(X_train,y_train,epochs=300,batch_size=20,validation_data=(X_test,y_test),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc0079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d577d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 6, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9095f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-09-23</th>\n",
       "      <td>39.299999</td>\n",
       "      <td>39.950001</td>\n",
       "      <td>39.200001</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>20518033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-24</th>\n",
       "      <td>40.200001</td>\n",
       "      <td>40.799999</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>19052618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-27</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>41.099998</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.299999</td>\n",
       "      <td>40.299999</td>\n",
       "      <td>20332913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-28</th>\n",
       "      <td>41.250000</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>40.849998</td>\n",
       "      <td>40.849998</td>\n",
       "      <td>29545281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-29</th>\n",
       "      <td>40.500000</td>\n",
       "      <td>41.599998</td>\n",
       "      <td>40.299999</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>30287737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume\n",
       "Date                                                                       \n",
       "2021-09-23  39.299999  39.950001  39.200001  39.750000  39.750000  20518033\n",
       "2021-09-24  40.200001  40.799999  39.700001  39.700001  39.700001  19052618\n",
       "2021-09-27  40.000000  41.099998  40.000000  40.299999  40.299999  20332913\n",
       "2021-09-28  41.250000  41.250000  40.599998  40.849998  40.849998  29545281\n",
       "2021-09-29  40.500000  41.599998  40.299999  41.250000  41.250000  30287737"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for the SPY ETF by specifying the stock ticker, start date, and end date\n",
    "df = yf.download('0005.hk',\"2021-09-22\",\"2021-09-30\")\n",
    "# data.drop(\"Adj Close\",axis=1,inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321c27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c73687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
