{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9ae2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score, mean_squared_error,mean_absolute_percentage_error,r2_score\n",
    "import tensorflow as tf\n",
    "import talib\n",
    "from tensorflow import keras\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad5f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "data=yf.download('0005.hk','2018-01-01','2022-04-09')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781ab9b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['6day MA'] = data['Close'].rolling(window = 6).mean()\n",
    "data['12day MA'] = data['Close'].rolling(window = 12).mean()\n",
    "data['RSI'] = talib.RSI(data['Close'].values, timeperiod = 7)\n",
    "data['%R5'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 5)\n",
    "data['%R10'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 10)\n",
    "data['MI6']=talib.MOM(data['Close'],timeperiod=6)\n",
    "data['MI12']=talib.MOM(data['Close'],timeperiod=12)\n",
    "macd, macdsignal, macdhist = talib.MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "data['EMA12']=talib.EMA(data['Close'], timeperiod=12)\n",
    "data['EMA26']=talib.EMA(data['Close'],timeperiod=26)\n",
    "data['MACD']=macd\n",
    "data['TR']=talib.TRANGE(data['High'],data['Low'],data['Close'])\n",
    "data['OSC6']=talib.CMO(data['Close'], timeperiod=6)\n",
    "data['OSC12']=talib.CMO(data['Close'], timeperiod=12)\n",
    "data['Prediction']=data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6d7cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>6day MA</th>\n",
       "      <th>12day MA</th>\n",
       "      <th>RSI</th>\n",
       "      <th>%R5</th>\n",
       "      <th>%R10</th>\n",
       "      <th>MI6</th>\n",
       "      <th>MI12</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD</th>\n",
       "      <th>TR</th>\n",
       "      <th>OSC6</th>\n",
       "      <th>OSC12</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-30</th>\n",
       "      <td>53.799999</td>\n",
       "      <td>54.450001</td>\n",
       "      <td>53.750000</td>\n",
       "      <td>53.950001</td>\n",
       "      <td>53.950001</td>\n",
       "      <td>20820331</td>\n",
       "      <td>53.308334</td>\n",
       "      <td>52.045834</td>\n",
       "      <td>69.919360</td>\n",
       "      <td>-20.833320</td>\n",
       "      <td>-10.638296</td>\n",
       "      <td>1.600002</td>\n",
       "      <td>4.350002</td>\n",
       "      <td>52.545739</td>\n",
       "      <td>52.523670</td>\n",
       "      <td>0.022069</td>\n",
       "      <td>0.950001</td>\n",
       "      <td>46.622377</td>\n",
       "      <td>18.475039</td>\n",
       "      <td>54.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31</th>\n",
       "      <td>53.950001</td>\n",
       "      <td>54.299999</td>\n",
       "      <td>53.799999</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>12951991</td>\n",
       "      <td>53.483334</td>\n",
       "      <td>52.462500</td>\n",
       "      <td>71.849555</td>\n",
       "      <td>-14.285714</td>\n",
       "      <td>-6.849312</td>\n",
       "      <td>1.049999</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>52.800241</td>\n",
       "      <td>52.647843</td>\n",
       "      <td>0.152398</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.893767</td>\n",
       "      <td>20.961286</td>\n",
       "      <td>54.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01</th>\n",
       "      <td>53.700001</td>\n",
       "      <td>54.349998</td>\n",
       "      <td>53.400002</td>\n",
       "      <td>54.349998</td>\n",
       "      <td>54.349998</td>\n",
       "      <td>8177523</td>\n",
       "      <td>53.758333</td>\n",
       "      <td>52.891667</td>\n",
       "      <td>73.059622</td>\n",
       "      <td>-6.060739</td>\n",
       "      <td>-2.777840</td>\n",
       "      <td>1.649998</td>\n",
       "      <td>5.149998</td>\n",
       "      <td>53.038665</td>\n",
       "      <td>52.773929</td>\n",
       "      <td>0.264737</td>\n",
       "      <td>0.949997</td>\n",
       "      <td>53.568896</td>\n",
       "      <td>22.508117</td>\n",
       "      <td>54.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-04</th>\n",
       "      <td>54.299999</td>\n",
       "      <td>54.349998</td>\n",
       "      <td>53.799999</td>\n",
       "      <td>54.049999</td>\n",
       "      <td>54.049999</td>\n",
       "      <td>10779509</td>\n",
       "      <td>53.966667</td>\n",
       "      <td>53.129167</td>\n",
       "      <td>66.399654</td>\n",
       "      <td>-24.242494</td>\n",
       "      <td>-12.903266</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.849998</td>\n",
       "      <td>53.194255</td>\n",
       "      <td>52.868452</td>\n",
       "      <td>0.325803</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>35.812143</td>\n",
       "      <td>17.491249</td>\n",
       "      <td>53.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-06</th>\n",
       "      <td>53.900002</td>\n",
       "      <td>54.049999</td>\n",
       "      <td>53.150002</td>\n",
       "      <td>53.700001</td>\n",
       "      <td>53.700001</td>\n",
       "      <td>17097274</td>\n",
       "      <td>53.958333</td>\n",
       "      <td>53.320833</td>\n",
       "      <td>59.070435</td>\n",
       "      <td>-57.692342</td>\n",
       "      <td>-31.249980</td>\n",
       "      <td>-0.049999</td>\n",
       "      <td>2.299999</td>\n",
       "      <td>53.272062</td>\n",
       "      <td>52.930049</td>\n",
       "      <td>0.342013</td>\n",
       "      <td>0.899998</td>\n",
       "      <td>16.890204</td>\n",
       "      <td>11.670984</td>\n",
       "      <td>53.450001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2022-03-30  53.799999  54.450001  53.750000  53.950001  53.950001  20820331   \n",
       "2022-03-31  53.950001  54.299999  53.799999  54.200001  54.200001  12951991   \n",
       "2022-04-01  53.700001  54.349998  53.400002  54.349998  54.349998   8177523   \n",
       "2022-04-04  54.299999  54.349998  53.799999  54.049999  54.049999  10779509   \n",
       "2022-04-06  53.900002  54.049999  53.150002  53.700001  53.700001  17097274   \n",
       "\n",
       "              6day MA   12day MA        RSI        %R5       %R10       MI6  \\\n",
       "Date                                                                          \n",
       "2022-03-30  53.308334  52.045834  69.919360 -20.833320 -10.638296  1.600002   \n",
       "2022-03-31  53.483334  52.462500  71.849555 -14.285714  -6.849312  1.049999   \n",
       "2022-04-01  53.758333  52.891667  73.059622  -6.060739  -2.777840  1.649998   \n",
       "2022-04-04  53.966667  53.129167  66.399654 -24.242494 -12.903266  1.250000   \n",
       "2022-04-06  53.958333  53.320833  59.070435 -57.692342 -31.249980 -0.049999   \n",
       "\n",
       "                MI12      EMA12      EMA26      MACD        TR       OSC6  \\\n",
       "Date                                                                        \n",
       "2022-03-30  4.350002  52.545739  52.523670  0.022069  0.950001  46.622377   \n",
       "2022-03-31  5.000000  52.800241  52.647843  0.152398  0.500000  50.893767   \n",
       "2022-04-01  5.149998  53.038665  52.773929  0.264737  0.949997  53.568896   \n",
       "2022-04-04  2.849998  53.194255  52.868452  0.325803  0.549999  35.812143   \n",
       "2022-04-06  2.299999  53.272062  52.930049  0.342013  0.899998  16.890204   \n",
       "\n",
       "                OSC12  Prediction  \n",
       "Date                               \n",
       "2022-03-30  18.475039   54.200001  \n",
       "2022-03-31  20.961286   54.349998  \n",
       "2022-04-01  22.508117   54.049999  \n",
       "2022-04-04  17.491249   53.700001  \n",
       "2022-04-06  11.670984   53.450001  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data=data[data.columns[list(backwardModel.k_feature_idx_)]]\n",
    "data['Prediction']=data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)\n",
    "data_val=data[-100:]\n",
    "data=data[:-100]\n",
    "data_val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b49c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_40899/237670532.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X=data.drop('Prediction',1)\n"
     ]
    }
   ],
   "source": [
    "sc = MinMaxScaler(feature_range = (-1, 1))\n",
    "# X=sc.fit_transform(data.drop('Prediction',1))\n",
    "X=data.drop('Prediction',1)\n",
    "Y = data[\"Prediction\"].values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b440227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/fyp_new/lib/python3.8/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "importance=mutual_info_regression(X,Y)\n",
    "feature_importances=pd.Series(importance,data.columns[0:len(data.columns)-1])\n",
    "feature_importances.plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6680b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(826, 1, 11) (826, 1) (92, 1, 11) (92, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w0jn89bj2w5g85gj4ck7w0t00000gn/T/ipykernel_40899/1506328268.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X=sc.fit_transform(data.drop(['Prediction','OSC12','OSC6','MI12','MI6','Volume','%R10','%R5','RSI'],1))\n"
     ]
    }
   ],
   "source": [
    "X=sc.fit_transform(data.drop(['Prediction','OSC12','OSC6','MI12','MI6','Volume','%R10','%R5','RSI'],1))\n",
    "# Y=np.array(data[\"Close\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8c5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 05:49:27.677069: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-11 05:49:27.677209: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "model_tech = keras.Sequential()\n",
    "model_tech.add(keras.layers.LSTM(\n",
    "  units=128,\n",
    "  input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    return_sequences=True\n",
    "    \n",
    "))\n",
    "model_tech.add(keras.layers.LSTM(units=64))\n",
    "model_tech.add(keras.layers.Dense(units=64))\n",
    "model_tech.add(keras.layers.Dense(units=1))\n",
    "model_tech.compile(\n",
    "  loss='mean_squared_error',\n",
    "  optimizer=keras.optimizers.Adam(0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d66a4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 05:49:28.687270: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-11 05:49:29.999937: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-11 05:49:30.200246: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-11 05:49:30.727450: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-11 05:49:31.997822: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-11 05:49:32.489564: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - ETA: 0s - loss: 3007.0964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 05:49:34.431886: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-11 05:49:34.502179: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-11 05:49:34.530794: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 6s 45ms/step - loss: 3007.0964 - val_loss: 2313.7319\n",
      "Epoch 2/500\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 1149.4187 - val_loss: 349.9315\n",
      "Epoch 3/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 100.4871 - val_loss: 35.3613\n",
      "Epoch 4/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 39.7111 - val_loss: 24.2634\n",
      "Epoch 5/500\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 30.9873 - val_loss: 18.6419\n",
      "Epoch 6/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 25.3456 - val_loss: 14.9772\n",
      "Epoch 7/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 21.2928 - val_loss: 12.3595\n",
      "Epoch 8/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 18.1482 - val_loss: 10.3867\n",
      "Epoch 9/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 15.5931 - val_loss: 8.8409\n",
      "Epoch 10/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 13.4582 - val_loss: 7.5945\n",
      "Epoch 11/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 11.6430 - val_loss: 6.5671\n",
      "Epoch 12/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 10.0812 - val_loss: 5.7050\n",
      "Epoch 13/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 8.7260 - val_loss: 4.9710\n",
      "Epoch 14/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 7.5431 - val_loss: 4.3388\n",
      "Epoch 15/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 6.5068 - val_loss: 3.7896\n",
      "Epoch 16/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 5.5975 - val_loss: 3.3098\n",
      "Epoch 17/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 4.8006 - val_loss: 2.8897\n",
      "Epoch 18/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 4.1048 - val_loss: 2.5226\n",
      "Epoch 19/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 3.5012 - val_loss: 2.2035\n",
      "Epoch 20/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 2.9825 - val_loss: 1.9289\n",
      "Epoch 21/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 2.5420 - val_loss: 1.6962\n",
      "Epoch 22/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 2.1735 - val_loss: 1.5030\n",
      "Epoch 23/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 1.8702 - val_loss: 1.3467\n",
      "Epoch 24/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 1.6251 - val_loss: 1.2242\n",
      "Epoch 25/500\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 1.4307 - val_loss: 1.1314\n",
      "Epoch 26/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 1.2793 - val_loss: 1.0636\n",
      "Epoch 27/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 1.1632 - val_loss: 1.0156\n",
      "Epoch 28/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 1.0752 - val_loss: 0.9826\n",
      "Epoch 29/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 1.0091 - val_loss: 0.9602\n",
      "Epoch 30/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.9594 - val_loss: 0.9447\n",
      "Epoch 31/500\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.9218 - val_loss: 0.9336\n",
      "Epoch 32/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.8929 - val_loss: 0.9249\n",
      "Epoch 33/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.8702 - val_loss: 0.9177\n",
      "Epoch 34/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.8520 - val_loss: 0.9111\n",
      "Epoch 35/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.8369 - val_loss: 0.9049\n",
      "Epoch 36/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.8242 - val_loss: 0.8988\n",
      "Epoch 37/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.8132 - val_loss: 0.8929\n",
      "Epoch 38/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.8037 - val_loss: 0.8871\n",
      "Epoch 39/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7953 - val_loss: 0.8814\n",
      "Epoch 40/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7879 - val_loss: 0.8759\n",
      "Epoch 41/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7813 - val_loss: 0.8705\n",
      "Epoch 42/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7756 - val_loss: 0.8652\n",
      "Epoch 43/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7705 - val_loss: 0.8601\n",
      "Epoch 44/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7662 - val_loss: 0.8552\n",
      "Epoch 45/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.7624 - val_loss: 0.8504\n",
      "Epoch 46/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7592 - val_loss: 0.8459\n",
      "Epoch 47/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7565 - val_loss: 0.8416\n",
      "Epoch 48/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7542 - val_loss: 0.8377\n",
      "Epoch 49/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7523 - val_loss: 0.8341\n",
      "Epoch 50/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7506 - val_loss: 0.8312\n",
      "Epoch 51/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7491 - val_loss: 0.8290\n",
      "Epoch 52/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7477 - val_loss: 0.8277\n",
      "Epoch 53/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.7463 - val_loss: 0.8274\n",
      "Epoch 54/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.7446 - val_loss: 0.8284\n",
      "Epoch 55/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7427 - val_loss: 0.8305\n",
      "Epoch 56/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7403 - val_loss: 0.8337\n",
      "Epoch 57/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7375 - val_loss: 0.8378\n",
      "Epoch 58/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7343 - val_loss: 0.8422\n",
      "Epoch 59/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7305 - val_loss: 0.8465\n",
      "Epoch 60/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7263 - val_loss: 0.8501\n",
      "Epoch 61/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.7218 - val_loss: 0.8525\n",
      "Epoch 62/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7171 - val_loss: 0.8535\n",
      "Epoch 63/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7123 - val_loss: 0.8528\n",
      "Epoch 64/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.7075 - val_loss: 0.8505\n",
      "Epoch 65/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.7029 - val_loss: 0.8468\n",
      "Epoch 66/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6985 - val_loss: 0.8419\n",
      "Epoch 67/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6945 - val_loss: 0.8362\n",
      "Epoch 68/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6908 - val_loss: 0.8299\n",
      "Epoch 69/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6875 - val_loss: 0.8232\n",
      "Epoch 70/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6846 - val_loss: 0.8164\n",
      "Epoch 71/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6822 - val_loss: 0.8096\n",
      "Epoch 72/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6801 - val_loss: 0.8030\n",
      "Epoch 73/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6785 - val_loss: 0.7965\n",
      "Epoch 74/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6773 - val_loss: 0.7903\n",
      "Epoch 75/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6764 - val_loss: 0.7844\n",
      "Epoch 76/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6759 - val_loss: 0.7788\n",
      "Epoch 77/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6757 - val_loss: 0.7736\n",
      "Epoch 78/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6758 - val_loss: 0.7688\n",
      "Epoch 79/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6762 - val_loss: 0.7643\n",
      "Epoch 80/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6767 - val_loss: 0.7603\n",
      "Epoch 81/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6774 - val_loss: 0.7566\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6781 - val_loss: 0.7534\n",
      "Epoch 83/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6788 - val_loss: 0.7504\n",
      "Epoch 84/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6793 - val_loss: 0.7477\n",
      "Epoch 85/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6795 - val_loss: 0.7451\n",
      "Epoch 86/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6794 - val_loss: 0.7426\n",
      "Epoch 87/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6789 - val_loss: 0.7401\n",
      "Epoch 88/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6780 - val_loss: 0.7376\n",
      "Epoch 89/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6766 - val_loss: 0.7351\n",
      "Epoch 90/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6748 - val_loss: 0.7327\n",
      "Epoch 91/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6727 - val_loss: 0.7303\n",
      "Epoch 92/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6703 - val_loss: 0.7282\n",
      "Epoch 93/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6677 - val_loss: 0.7262\n",
      "Epoch 94/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6651 - val_loss: 0.7246\n",
      "Epoch 95/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6625 - val_loss: 0.7233\n",
      "Epoch 96/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6601 - val_loss: 0.7222\n",
      "Epoch 97/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6578 - val_loss: 0.7212\n",
      "Epoch 98/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6558 - val_loss: 0.7204\n",
      "Epoch 99/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6539 - val_loss: 0.7195\n",
      "Epoch 100/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6524 - val_loss: 0.7186\n",
      "Epoch 101/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6510 - val_loss: 0.7176\n",
      "Epoch 102/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6498 - val_loss: 0.7164\n",
      "Epoch 103/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6488 - val_loss: 0.7150\n",
      "Epoch 104/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6480 - val_loss: 0.7135\n",
      "Epoch 105/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6473 - val_loss: 0.7118\n",
      "Epoch 106/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6467 - val_loss: 0.7099\n",
      "Epoch 107/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6462 - val_loss: 0.7079\n",
      "Epoch 108/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6458 - val_loss: 0.7058\n",
      "Epoch 109/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6455 - val_loss: 0.7035\n",
      "Epoch 110/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6452 - val_loss: 0.7011\n",
      "Epoch 111/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6450 - val_loss: 0.6987\n",
      "Epoch 112/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6448 - val_loss: 0.6961\n",
      "Epoch 113/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6446 - val_loss: 0.6936\n",
      "Epoch 114/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6445 - val_loss: 0.6910\n",
      "Epoch 115/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6444 - val_loss: 0.6884\n",
      "Epoch 116/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6444 - val_loss: 0.6859\n",
      "Epoch 117/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6443 - val_loss: 0.6835\n",
      "Epoch 118/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6443 - val_loss: 0.6811\n",
      "Epoch 119/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6442 - val_loss: 0.6789\n",
      "Epoch 120/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6442 - val_loss: 0.6768\n",
      "Epoch 121/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6441 - val_loss: 0.6749\n",
      "Epoch 122/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6441 - val_loss: 0.6731\n",
      "Epoch 123/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6440 - val_loss: 0.6715\n",
      "Epoch 124/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6440 - val_loss: 0.6700\n",
      "Epoch 125/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6439 - val_loss: 0.6687\n",
      "Epoch 126/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6439 - val_loss: 0.6675\n",
      "Epoch 127/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6439 - val_loss: 0.6665\n",
      "Epoch 128/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6438 - val_loss: 0.6655\n",
      "Epoch 129/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6438 - val_loss: 0.6647\n",
      "Epoch 130/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6438 - val_loss: 0.6639\n",
      "Epoch 131/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6437 - val_loss: 0.6633\n",
      "Epoch 132/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6437 - val_loss: 0.6627\n",
      "Epoch 133/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6437 - val_loss: 0.6622\n",
      "Epoch 134/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6436 - val_loss: 0.6617\n",
      "Epoch 135/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6436 - val_loss: 0.6614\n",
      "Epoch 136/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6435 - val_loss: 0.6610\n",
      "Epoch 137/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6435 - val_loss: 0.6608\n",
      "Epoch 138/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6434 - val_loss: 0.6606\n",
      "Epoch 139/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6433 - val_loss: 0.6604\n",
      "Epoch 140/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6432 - val_loss: 0.6603\n",
      "Epoch 141/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6431 - val_loss: 0.6603\n",
      "Epoch 142/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6430 - val_loss: 0.6602\n",
      "Epoch 143/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6428 - val_loss: 0.6603\n",
      "Epoch 144/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6426 - val_loss: 0.6603\n",
      "Epoch 145/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6425 - val_loss: 0.6604\n",
      "Epoch 146/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6423 - val_loss: 0.6605\n",
      "Epoch 147/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6420 - val_loss: 0.6607\n",
      "Epoch 148/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6418 - val_loss: 0.6609\n",
      "Epoch 149/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6416 - val_loss: 0.6611\n",
      "Epoch 150/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6413 - val_loss: 0.6613\n",
      "Epoch 151/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6410 - val_loss: 0.6616\n",
      "Epoch 152/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6407 - val_loss: 0.6618\n",
      "Epoch 153/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6404 - val_loss: 0.6621\n",
      "Epoch 154/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6401 - val_loss: 0.6624\n",
      "Epoch 155/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6398 - val_loss: 0.6628\n",
      "Epoch 156/500\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.6394 - val_loss: 0.6631\n",
      "Epoch 157/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6391 - val_loss: 0.6634\n",
      "Epoch 158/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6387 - val_loss: 0.6638\n",
      "Epoch 159/500\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.6383 - val_loss: 0.6641\n",
      "Epoch 160/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6380 - val_loss: 0.6645\n",
      "Epoch 161/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6376 - val_loss: 0.6649\n",
      "Epoch 162/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6372 - val_loss: 0.6653\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6368 - val_loss: 0.6656\n",
      "Epoch 164/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6363 - val_loss: 0.6660\n",
      "Epoch 165/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6359 - val_loss: 0.6664\n",
      "Epoch 166/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6355 - val_loss: 0.6668\n",
      "Epoch 167/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6351 - val_loss: 0.6671\n",
      "Epoch 168/500\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.6346 - val_loss: 0.6675\n",
      "Epoch 169/500\n",
      " 1/47 [..............................] - ETA: 0s - loss: 1.1681"
     ]
    }
   ],
   "source": [
    "history_tech=model_tech.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "y_pred = model_tech.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_tech.history['loss'], label='train')\n",
    "plt.plot(history_tech.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(y_train)), y_train, 'g', label=\"history\")\n",
    "plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test, marker='.', label=\"true\")\n",
    "plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred, 'r', label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134553b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_test = sc.inverse_transform(y_test)\n",
    "# y_pred = sc.inverse_transform(y_pred)\n",
    "plt.plot(y_test, marker='.', label=\"true\")\n",
    "plt.plot(y_pred, 'r', label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf212a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,y_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=data_val.drop(['Prediction','OSC12','OSC6','MI12','MI6','Volume','%R10','%R5','RSI'],1)\n",
    "y_val=data_val['Prediction']\n",
    "x_val=sc.transform(x_val).reshape(x_val.shape[0],1,x_val.shape[1])\n",
    "y_val_pred=model_tech.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529aefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_val.values, marker='.', label=\"true\")\n",
    "plt.plot(y_val_pred, 'r', marker='*',label=\"prediction\")\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time Step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_val.values,y_val_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab65e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
